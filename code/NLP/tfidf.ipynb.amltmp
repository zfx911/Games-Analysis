{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In interactive notebook, the `spark` object is already created.\n",
        "Instructors tested with 1 driver, 6 executors of small e4 (24 cores, 192GB memory)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch spark environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "53",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:42:40.8980971Z",
              "session_start_time": "2023-11-19T21:42:40.9325348Z",
              "execution_start_time": "2023-11-19T21:45:32.852718Z",
              "execution_finish_time": "2023-11-19T21:45:35.3957748Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "3b27e0cf-63ef-4416-b4de-52860604dec8"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 53, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f6a0b0048e0>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-2ad45899:38013\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.2.5.1-100879434</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430334632
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f \\\n",
        "{\"conf\": {\"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.2\"}}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:42:40.9080311Z",
              "session_start_time": "2023-11-19T21:45:35.5286211Z",
              "execution_start_time": "2023-11-19T21:47:31.5640875Z",
              "execution_finish_time": "2023-11-19T21:47:31.5920629Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "3bdad02b-63ef-4b02-896b-a331f6323e20"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Unrecognized options: "
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up data configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob_account_name = \"marckvnonprodblob\"\n",
        "blob_container_name = \"bigdata\"\n",
        "# read only\n",
        "blob_sas_token = \"?sv=2021-10-04&st=2023-10-04T01%3A42%3A59Z&se=2024-01-02T02%3A42%3A00Z&sr=c&sp=rlf&sig=w3CH9MbCOpwO7DtHlrahc7AlRPxSZZb8MOgS6TaXLzI%3D\"\n",
        "\n",
        "wasbs_base_url = (\n",
        "    f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/\"\n",
        ")\n",
        "spark.conf.set(\n",
        "    f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\",\n",
        "    blob_sas_token,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:42:40.9089622Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:47:51.2783526Z",
              "execution_finish_time": "2023-11-19T21:47:51.5943384Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "09799b7f-0465-4588-8e2b-96fa19d85129"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430470738
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading in single parquet file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_path = \"reddit-parquet/comments/\"\n",
        "submissions_path = \"reddit-parquet/submissions/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:42:44.0710898Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:47:51.6959686Z",
              "execution_finish_time": "2023-11-19T21:47:52.0146377Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9c12bca3-b685-4cde-8ac5-079b561d48fd"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430471117
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\"Tetris\",\"pokemon\",\"SuperMario\",\"GTA\",\"CallOfDuty\",\"FIFA\",\"legostarwars\",\n",
        "\"assassinscreed\",\"thesims\",\"FinalFantasy\"] "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:42:45.774678Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:47:52.1394762Z",
              "execution_finish_time": "2023-11-19T21:47:52.4409596Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "817a05c2-74d9-4bb9-b5dc-55d6741741ac"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430471582
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reeading in all of the Reddit data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\n",
        "submissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:42:47.7653659Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:47:52.5502425Z",
              "execution_finish_time": "2023-11-19T21:48:34.210854Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2023-11-19T21:48:26.092GMT",
                    "completionTime": "2023-11-19T21:48:32.013GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2023-11-19T21:48:10.769GMT",
                    "completionTime": "2023-11-19T21:48:17.578GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "af40d9c9-2db5-4170-9366-8d0061571612"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430513322
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length, col,split\n",
        "sub_filtered = submissions_df.filter((length(col(\"title\")) > 0)& (col(\"title\") != \"[deleted]\")&(col('title')!= \"[removed]\"))\\\n",
        ".filter(col(\"subreddit\").isin(topic))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:42:49.7965168Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:48:34.3214551Z",
              "execution_finish_time": "2023-11-19T21:48:35.12355Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "418c4656-5c10-47a5-9e06-a4d2a480962e"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430514215
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_save = sub_filtered.select(\"subreddit\", \"title\",\"year\",\"month\").sample(fraction= 0.2,seed =22).cache()\n",
        "df_save.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:42:52.2244307Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:48:35.2337047Z",
              "execution_finish_time": "2023-11-19T21:48:45.2382341Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 9698162,
                    "rowCount": 221807,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\ndf_save = sub_filtered.select(\"subreddit\", \"title\",\"year\",\"month\").sample(fraction= 0.2,seed =22).cache()\ndf_save.show()",
                    "submissionTime": "2023-11-19T21:48:37.529GMT",
                    "completionTime": "2023-11-19T21:48:43.117GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9b0ab5f6-13ef-450f-bdaf-385e8f87ce65"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+----+-----+\n|     subreddit|               title|year|month|\n+--------------+--------------------+----+-----+\n|       pokemon|the PokemonTogeth...|2023|    2|\n|       pokemon|Who's a non-villa...|2023|    2|\n|       thesims|(If we know) when...|2023|    2|\n|    CallOfDuty|SBMM was very nic...|2023|    2|\n|           GTA|Is GTA china town...|2023|    2|\n|assassinscreed|Network issues wi...|2023|    2|\n|       pokemon|Opinion on Pokemo...|2023|    2|\n|       pokemon|     Team Gyarados🥶|2023|    2|\n|       pokemon|I made Tinkaton o...|2023|    2|\n|       pokemon|        Vaporeon...?|2023|    2|\n|  legostarwars|how much is this ...|2023|    2|\n|       pokemon|One of the follow...|2023|    2|\n|          FIFA|89 Ben Yedder or ...|2023|    2|\n|       pokemon|  Pawmo not evolving|2023|    2|\n|       pokemon|Factually the cut...|2023|    2|\n|       pokemon|Main Series games...|2023|    2|\n|          FIFA|Mbappe &amp; Rttf...|2023|    2|\n|           GTA|I used faceapp fo...|2023|    2|\n|  FinalFantasy|Advent Chldren In...|2023|    2|\n|       pokemon|First time EV tra...|2023|    2|\n+--------------+--------------------+----+-----+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430524356
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_save.rdd.getNumPartitions()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:49:07.6535912Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:49:07.7624908Z",
              "execution_finish_time": "2023-11-19T21:49:08.0613319Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "63b2fdb9-62e4-474f-9ba8-44f5901e69ab"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "3080"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430547191
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using TFIDF to identify the key points for each game "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:49:13.1734719Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:49:13.2658209Z",
              "execution_finish_time": "2023-11-19T21:49:27.4517192Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "a46bca02-3d20-414c-8808-9cc682c3efe9"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting spark-nlp\n  Downloading spark_nlp-5.1.4-py2.py3-none-any.whl (540 kB)\n\u001b[K     |████████████████████████████████| 540 kB 9.7 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: spark-nlp\nSuccessfully installed spark-nlp-5.1.4\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer as tot, StopWordsRemover\n",
        "from pyspark.sql.functions import length, col,split"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:49:19.5821861Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:49:27.5621941Z",
              "execution_finish_time": "2023-11-19T21:49:31.0027333Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "706f2d03-4479-4cb2-bee2-0dfeaeb468da"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 15, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430570118
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol('title') \\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "cleanUpPatterns = [\"<[^>]*>\"]\n",
        "# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\n",
        "documentNormalizer = DocumentNormalizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"normalizedDocument\") \\\n",
        "    .setAction(\"clean\") \\\n",
        "    .setPatterns(cleanUpPatterns) \\\n",
        "    .setReplacement(\" \") \\\n",
        "    .setPolicy(\"pretty_all\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "sentenceDetector = SentenceDetector() \\\n",
        "      .setInputCols([\"normalizedDocument\"]) \\\n",
        "      .setOutputCol(\"sentence\")\n",
        "'''\n",
        "regexTokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"sentence\"]) \\\n",
        "      .setOutputCol(\"token\") \\\n",
        "      .fit(dataeg)\n",
        "'''\n",
        "# tokenization\n",
        "tokenizer = (\n",
        "    Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        ")\n",
        "\n",
        "# make the words back to root form\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "\n",
        "# remove stop words\n",
        "stop_words_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"stem\"])  \\\n",
        "    .setOutputCol(\"cleaned_token\") \\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "# check spelling\n",
        "spellModel = ContextSpellCheckerModel\\\n",
        "    .pretrained(\"spellcheck_dl\", \"en\")\\\n",
        "    .setInputCols(\"cleaned_token\")\\\n",
        "    .setOutputCol(\"final_token\")\\\n",
        "\n",
        "nlpcleanPipeline = \\\n",
        "  Pipeline() \\\n",
        "    .setStages([\n",
        "        documentAssembler,\n",
        "        documentNormalizer,\n",
        "        sentenceDetector,\n",
        "        tokenizer,\n",
        "        stemmer,\n",
        "        stop_words_cleaner,\n",
        "        spellModel])\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:49:23.0801468Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:49:31.1078411Z",
              "execution_finish_time": "2023-11-19T21:50:50.279147Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 10
                },
                "jobs": [
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 1582672,
                    "rowCount": 2840,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:34.827GMT",
                    "completionTime": "2023-11-19T21:50:35.002GMT",
                    "stageIds": [
                      12
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 1253530,
                    "rowCount": 2840,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:34.273GMT",
                    "completionTime": "2023-11-19T21:50:34.486GMT",
                    "stageIds": [
                      11
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 1253530,
                    "rowCount": 2840,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:33.797GMT",
                    "completionTime": "2023-11-19T21:50:34.022GMT",
                    "stageIds": [
                      10
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 1364274,
                    "rowCount": 2840,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:33.073GMT",
                    "completionTime": "2023-11-19T21:50:33.418GMT",
                    "stageIds": [
                      9
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at SpecialTokensParser.scala:72",
                    "dataWritten": 0,
                    "dataRead": 36685113,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "collect at SpecialTokensParser.scala:72",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:13.652GMT",
                    "completionTime": "2023-11-19T21:50:31.562GMT",
                    "stageIds": [
                      8
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 9,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 9,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at SpecialTokensParser.scala:72",
                    "dataWritten": 0,
                    "dataRead": 367997,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "collect at SpecialTokensParser.scala:72",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:13.218GMT",
                    "completionTime": "2023-11-19T21:50:13.560GMT",
                    "stageIds": [
                      7
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 9,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 9,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at SpecialTokensParser.scala:72",
                    "dataWritten": 0,
                    "dataRead": 1898328,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "collect at SpecialTokensParser.scala:72",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:11.662GMT",
                    "completionTime": "2023-11-19T21:50:13.111GMT",
                    "stageIds": [
                      6
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 9,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 9,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at SpecialTokensParser.scala:72",
                    "dataWritten": 0,
                    "dataRead": 464508,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 5,
                    "name": "collect at SpecialTokensParser.scala:72",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:11.038GMT",
                    "completionTime": "2023-11-19T21:50:11.536GMT",
                    "stageIds": [
                      5
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 9,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 9,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:395",
                    "dataWritten": 0,
                    "dataRead": 511577,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "collect at Feature.scala:395",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:09.338GMT",
                    "completionTime": "2023-11-19T21:50:10.571GMT",
                    "stageIds": [
                      4
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 507,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 16:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-19T21:50:08.656GMT",
                    "completionTime": "2023-11-19T21:50:09.164GMT",
                    "stageIds": [
                      3
                    ],
                    "jobGroup": "16",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "aa665d04-148e-41e6-83fa-bb72f997d58c"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "spellcheck_dl download started this may take some time.\nApproximate size to download 95.1 MB\n[OK!]\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430649458
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = nlpcleanPipeline.fit(df_save).transform(df_save)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:49:29.0522948Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:50:50.4075879Z",
              "execution_finish_time": "2023-11-19T21:50:51.9587135Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "7c42beda-60b4-459e-95aa-518e98e67159"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 17, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430651039
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size, concat_ws\n",
        "new_df_post = cleaned_df.select(\"subreddit\", 'final_token.result')\n",
        "# remove empty array\n",
        "new_df_post_filter = new_df_post.filter(size(col(\"result\")) > 0)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:49:30.7337344Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:50:52.0887903Z",
              "execution_finish_time": "2023-11-19T21:50:52.4011245Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "150ed629-e858-4321-9faf-18e0a515dbd7"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 18, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430651582
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = new_df_post_filter.withColumn(\"text\", concat_ws(\", \", col(\"result\")))\n",
        "final_df_post = final_df.select(\"subreddit\", 'text')\n",
        "final_df_post.show(10)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:49:44.3690093Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:50:52.5175791Z",
              "execution_finish_time": "2023-11-19T21:51:24.5723064Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 3152,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 19:\nfinal_df = new_df_post_filter.withColumn(\"text\", concat_ws(\", \", col(\"result\")))\nfinal_df_post = final_df.select(\"subreddit\", 'text')\nfinal_df_post.show(10)",
                    "submissionTime": "2023-11-19T21:50:53.065GMT",
                    "completionTime": "2023-11-19T21:51:23.860GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "19",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "2c21d863-81b8-4ade-83db-865575c6a7d4"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 19, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+\n|     subreddit|                text|\n+--------------+--------------------+\n|       pokemon|pokemontogeth, ca...|\n|       pokemon|who, non-villain,...|\n|       thesims|thank, !, (, know...|\n|    CallOfDuty|same, wa, very, n...|\n|           GTA|Uta, china, town,...|\n|assassinscreed|network, issue, u...|\n|       pokemon|opinion, common, ...|\n|       pokemon|    team, gyarados🥶|\n|       pokemon|Doc, made, Pinkst...|\n|       pokemon|?, ., ., Napoleon, .|\n+--------------+--------------------+\nonly showing top 10 rows\n\n"
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430683724
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df_post.rdd.getNumPartitions()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 20,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:50:07.9187494Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:51:24.6919896Z",
              "execution_finish_time": "2023-11-19T21:51:24.9905843Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "df9ba2e9-e8f4-4592-aa6d-ec4fdd2f14aa"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 20, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 39,
          "data": {
            "text/plain": "3080"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430684329
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = DocumentAssembler() \\\n",
        ".setInputCol(\"text\") \\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "useEmbeddings = UniversalSentenceEncoder.pretrained() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "\n",
        "sentiment = SentimentDLModel.pretrained(\"sentimentdl_use_twitter\") \\\n",
        "    .setInputCols([\"sentence_embeddings\"]) \\\n",
        "    .setThreshold(0.7) \\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "\n",
        "pipeline = Pipeline().setStages([document, useEmbeddings, sentiment])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 35,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T22:02:41.0239394Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T22:02:41.1235732Z",
              "execution_finish_time": "2023-11-19T22:04:43.0856154Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 346,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 28,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 35:\ndocument = DocumentAssembler() .setInputCol(\"text\") .setOutputCol(\"document\")\n\nuseEmbeddings = UniversalSentenceEncoder.pretrained()     .setInputCols([\"document\"])     .setOutputCol(\"sentence_embeddings\")\n\n\nsentiment = SentimentDLModel.pretrained(\"sentimentdl_use_twitter\")     .setInputCols([\"sentence_embeddings\"])     .setThreshold(0.7)     .setOutputCol(\"sentiment\")\n\npipeline = Pipeline().setStages([document, useEmbeddings, sentiment])",
                    "submissionTime": "2023-11-19T22:03:42.358GMT",
                    "completionTime": "2023-11-19T22:03:42.441GMT",
                    "stageIds": [
                      28
                    ],
                    "jobGroup": "35",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "b4160660-169b-4145-bd51-194e922dbd33"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 35, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n[OK!]\nsentimentdl_use_twitter download started this may take some time.\nApproximate size to download 11.4 MB\n[OK!]\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700431482239
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.fit(final_df_post).transform(final_df_post).cache()\n",
        "result.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 40,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T22:09:36.2885813Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T22:09:36.3958768Z",
              "execution_finish_time": "2023-11-19T22:09:52.8202099Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 3152,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 30,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 40:\nresult = pipeline.fit(final_df_post).transform(final_df_post).cache()\nresult.show()",
                    "submissionTime": "2023-11-19T22:09:36.954GMT",
                    "completionTime": "2023-11-19T22:09:51.275GMT",
                    "stageIds": [
                      30
                    ],
                    "jobGroup": "40",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "a7c3cca6-68bb-4bbd-a268-ece353358eaf"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 40, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+--------------------+--------------------+--------------------+\n|     subreddit|                text|            document| sentence_embeddings|           sentiment|\n+--------------+--------------------+--------------------+--------------------+--------------------+\n|       pokemon|pokemontogeth, ca...|[{document, 0, 22...|[{sentence_embedd...|[{category, 0, 22...|\n|       pokemon|who, non-villain,...|[{document, 0, 42...|[{sentence_embedd...|[{category, 0, 42...|\n|       thesims|thank, !, (, know...|[{document, 0, 50...|[{sentence_embedd...|[{category, 0, 50...|\n|    CallOfDuty|same, wa, very, n...|[{document, 0, 32...|[{sentence_embedd...|[{category, 0, 32...|\n|           GTA|Uta, china, town,...|[{document, 0, 39...|[{sentence_embedd...|[{category, 0, 39...|\n|assassinscreed|network, issue, u...|[{document, 0, 20...|[{sentence_embedd...|[{category, 0, 20...|\n|       pokemon|opinion, common, ...|[{document, 0, 23...|[{sentence_embedd...|[{category, 0, 23...|\n|       pokemon|    team, gyarados🥶|[{document, 0, 15...|[{sentence_embedd...|[{category, 0, 15...|\n|       pokemon|Doc, made, Pinkst...|[{document, 0, 27...|[{sentence_embedd...|[{category, 0, 27...|\n|       pokemon|?, ., ., Napoleon, .|[{document, 0, 19...|[{sentence_embedd...|[{category, 0, 19...|\n|  legostarwars|much, the, coli, ...|[{document, 0, 46...|[{sentence_embedd...|[{category, 0, 46...|\n|       pokemon|follow, common, f...|[{document, 0, 60...|[{sentence_embedd...|[{category, 0, 60...|\n|          FIFA|89, men, leader, ...|[{document, 0, 36...|[{sentence_embedd...|[{category, 0, 36...|\n|       pokemon|         paw, evolve|[{document, 0, 10...|[{sentence_embedd...|[{category, 0, 10...|\n|       pokemon|factual, test, co...|[{document, 0, 40...|[{sentence_embedd...|[{category, 0, 40...|\n|       pokemon|main, Geri, game,...|[{document, 0, 33...|[{sentence_embedd...|[{category, 0, 33...|\n|          FIFA|sell, ?, roof, Ph...|[{document, 0, 10...|[{sentence_embedd...|[{category, 0, 10...|\n|           GTA|     us, face, smile|[{document, 0, 14...|[{sentence_embedd...|[{category, 0, 14...|\n|  FinalFantasy|advent, children,...|[{document, 0, 53...|[{sentence_embedd...|[{category, 0, 53...|\n|       pokemon|first, time, ve, ...|[{document, 0, 21...|[{sentence_embedd...|[{category, 0, 21...|\n+--------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700431791906
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.withColumn(\"sentiment\",F.explode(\"sentiment.result\"))\n",
        "result = result.select(\"subreddit\",\"text\",\"sentiment\")\n",
        "result.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 41,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T22:10:11.8756763Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T22:10:11.988449Z",
              "execution_finish_time": "2023-11-19T22:10:12.798981Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 132120,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 31,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 41:\nresult = result.withColumn(\"sentiment\",F.explode(\"sentiment.result\"))\nresult = result.select(\"subreddit\",\"text\",\"sentiment\")\nresult.show()",
                    "submissionTime": "2023-11-19T22:10:12.181GMT",
                    "completionTime": "2023-11-19T22:10:12.283GMT",
                    "stageIds": [
                      31
                    ],
                    "jobGroup": "41",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "876f94cc-53b9-4c1e-b631-5feab747ceaa"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 41, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+---------+\n|     subreddit|                text|sentiment|\n+--------------+--------------------+---------+\n|       pokemon|pokemontogeth, ca...| positive|\n|       pokemon|who, non-villain,...| negative|\n|       thesims|thank, !, (, know...| positive|\n|    CallOfDuty|same, wa, very, n...| positive|\n|           GTA|Uta, china, town,...| positive|\n|assassinscreed|network, issue, u...| negative|\n|       pokemon|opinion, common, ...| positive|\n|       pokemon|    team, gyarados🥶| positive|\n|       pokemon|Doc, made, Pinkst...| positive|\n|       pokemon|?, ., ., Napoleon, .| positive|\n|  legostarwars|much, the, coli, ...| positive|\n|       pokemon|follow, common, f...| positive|\n|          FIFA|89, men, leader, ...| positive|\n|       pokemon|         paw, evolve| positive|\n|       pokemon|factual, test, co...| positive|\n|       pokemon|main, Geri, game,...|  neutral|\n|          FIFA|sell, ?, roof, Ph...| positive|\n|           GTA|     us, face, smile| positive|\n|  FinalFantasy|advent, children,...| positive|\n|       pokemon|first, time, ve, ...| negative|\n+--------------+--------------------+---------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700431811878
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, regexp_replace\n",
        "df = result.withColumn(\"text\", regexp_replace(col(\"text\"), \",\", \"\"))\n",
        "\n",
        "# Remove parentheses from the sentiment\n",
        "df = df.withColumn(\"final_sentiment\", col(\"final_sentiment\")[0])\n",
        "df = df.drop(\"text\")\n",
        "df = df.filter(col(\"final_sentiment\") != \"na\")\n",
        "# Show the resulting DataFrame\n",
        "df = df.coalesce(3080)\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:54:00.9480635Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:54:01.2835987Z",
              "execution_finish_time": "2023-11-19T21:54:02.8016745Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 4184,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 19,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 25:\nfrom pyspark.sql.functions import col, regexp_replace\ndf = result.withColumn(\"text\", regexp_replace(col(\"text\"), \",\", \"\"))\n\n# Remove parentheses from the sentiment\ndf = df.withColumn(\"final_sentiment\", col(\"final_sentiment\")[0])\ndf = df.drop(\"text\")\ndf = df.filter(col(\"final_sentiment\") != \"na\")\n# Show the resulting DataFrame\ndf = df.coalesce(3080)\ndf.show()",
                    "submissionTime": "2023-11-19T21:54:01.694GMT",
                    "completionTime": "2023-11-19T21:54:01.970GMT",
                    "stageIds": [
                      19
                    ],
                    "jobGroup": "25",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "982b9325-f008-4ccf-9e58-941065fea2a1"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 25, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+------------+---------------+\n|   subreddit|final_sentiment|\n+------------+---------------+\n|     pokemon|       negative|\n|     pokemon|       negative|\n|     thesims|       negative|\n|  CallOfDuty|       positive|\n|         GTA|       negative|\n|     pokemon|       negative|\n|     pokemon|       positive|\n|     pokemon|       negative|\n|legostarwars|       positive|\n|     pokemon|       negative|\n|        FIFA|       negative|\n|     pokemon|       positive|\n|     pokemon|       positive|\n|        FIFA|       negative|\n|         GTA|       negative|\n|FinalFantasy|       positive|\n|     pokemon|       positive|\n|        FIFA|       negative|\n|        FIFA|       positive|\n|legostarwars|       negative|\n+------------+---------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700430841935
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T21:58:44.9452512Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:58:45.0418133Z",
              "execution_finish_time": "2023-11-19T21:58:45.3386131Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "30ba79e3-11dc-472c-a6a1-696da00262c9"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 31, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 61,
          "data": {
            "text/plain": "3080"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700431124548
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(fraction=0.2,seed= 20).cache()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 33,
              "state": "cancelled",
              "livy_statement_state": "waiting",
              "queued_time": "2023-11-19T21:59:17.0448123Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T21:59:17.1551618Z",
              "execution_finish_time": "2023-11-19T21:59:39.3523728Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 1,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [
                  {
                    "displayName": "count at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 2714,
                    "dataRead": 80189857,
                    "rowCount": 1852423,
                    "usageDescription": "",
                    "jobId": 24,
                    "name": "count at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 33:\ndf = df.sample(fraction=0.2,seed= 20).cache()\ndf_save.count()",
                    "submissionTime": "2023-11-19T21:59:17.209GMT",
                    "stageIds": [
                      24
                    ],
                    "jobGroup": "33",
                    "status": "RUNNING",
                    "numTasks": 3080,
                    "numActiveTasks": 7,
                    "numCompletedTasks": 46,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 46,
                    "numActiveStages": 1,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9bc8c273-b4e4-4c46-945a-7b1cc4988b4a"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 33, Cancelled, Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700431178213
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.sample(fraction=0.4,seed=22).cache()\n",
        "re"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 43,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-19T22:12:11.3834001Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T22:12:11.4748478Z",
              "execution_finish_time": "2023-11-19T22:12:11.7937296Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "3bfd1bed-9765-4a33-b72d-300e8782ddfb"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 43, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700431930872
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "result.toPandas().to_csv(f\"{CSV_DIR}/sentiment_tfidf2.csv\",index=False)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "54",
              "statement_id": 44,
              "state": "cancelled",
              "livy_statement_state": "waiting",
              "queued_time": "2023-11-19T22:12:13.5430397Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-19T22:12:13.8671563Z",
              "execution_finish_time": "2023-11-19T22:19:42.7412707Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 1,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [
                  {
                    "displayName": "toPandas at /tmp/ipykernel_10534/2551388859.py:3",
                    "dataWritten": 0,
                    "dataRead": 472224976,
                    "rowCount": 10900383,
                    "usageDescription": "",
                    "jobId": 33,
                    "name": "toPandas at /tmp/ipykernel_10534/2551388859.py:3",
                    "description": "Job group for statement 44:\nimport os\nCSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\nresult.toPandas().to_csv(f\"{CSV_DIR}/sentiment_tfidf2.csv\",index=False)",
                    "submissionTime": "2023-11-19T22:12:14.049GMT",
                    "stageIds": [
                      33
                    ],
                    "jobGroup": "44",
                    "status": "RUNNING",
                    "numTasks": 3080,
                    "numActiveTasks": 9,
                    "numCompletedTasks": 99,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 99,
                    "numActiveStages": 1,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "ba41be21-5154-495b-bc26-a0c58b99cb54"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 54, 44, Cancelled, Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700432381653
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.cache()\n",
        "ndf.select(\"subreddit\",\"expwords\",\"year\",\"month\",\"sentiment\",\"wordhash\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:25:19.1803753Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:25:19.2885858Z",
              "execution_finish_time": "2023-11-16T11:25:19.62201Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "80d83379-6a1c-4080-87d4-a4cdb90a796f"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 31, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 61,
          "data": {
            "text/plain": "DataFrame[subreddit: string, expwords: string, year: int, month: int, sentiment: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, wordhash: int]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700133919568
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "udf1 = f.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
        "valuedf = result.select('subreddit',\"filtered_tokens\",\"year\",\"month\",\"sentiment\",f.explode(udf1(f.col('features'))).name('wordhash','value'))\n",
        "valuedf = valuedf.withColumn(\"sentiment\",f.explode(\"sentiment.result\"))\n",
        "valuedf = valuedf.drop_duplicates(subset=[\"wordhash\"])\n",
        "valuedf.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 36,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:43:07.6492253Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:43:07.8041378Z",
              "execution_finish_time": "2023-11-16T11:43:09.3126611Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "371a5faf-db3d-4bd7-bc8d-130b561bccc6"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 36, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---------+--------------------+----+-----+---------+--------+------------------+\n|subreddit|     filtered_tokens|year|month|sentiment|wordhash|             value|\n+---------+--------------------+----+-----+---------+--------+------------------+\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  139265| 9.740140549187382|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   88005| 5.463474430171326|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  215686| 6.332122839048911|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  114628|10.992903517682748|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   63750| 7.041659799101322|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   80646| 20.59951267424561|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   12999|10.992903517682748|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  186058| 5.152261860309351|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  113673|2.5955078897221076|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  152651|  7.76407736196138|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  153551| 19.34229535540086|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   62030| 9.228105951733017|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   31064| 6.471114940633709|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  215514|6.2328686211337425|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  110813|  2.45101771367614|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   98717| 2.479417188197838|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   29214| 6.138922033631871|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  249180|0.5217682340853889|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|    4959|4.4497113554883505|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  123426| 5.801336521081593|\n+---------+--------------------+----+-----+---------+--------+------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700134989254
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valuedf = valuedf.drop_duplicates(subset=[\"subreddit\",\"wordhash\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 38,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:46:10.1396779Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:46:10.2543451Z",
              "execution_finish_time": "2023-11-16T11:46:10.5786422Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "2d88c823-4825-407b-9180-2931739a3ee3"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 38, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700135170488
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valuedf.cache()\n",
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "valuedf.toPandas().to_csv(f\"{CSV_DIR}/analysis-2.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 39,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2023-11-16T11:46:15.2932564Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:46:15.4216838Z",
              "execution_finish_time": null,
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "de45e2eb-810f-4b1e-a631-0099ab02b29a"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 39, Submitted, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700135130854
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.join(valuedf,['subreddit','wordhash'],\"right_outer\").cache()\n",
        "result_df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 33,
              "state": "cancelled",
              "livy_statement_state": "waiting",
              "queued_time": "2023-11-16T11:25:52.5189687Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:25:52.6183166Z",
              "execution_finish_time": "2023-11-16T11:41:03.4114082Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 1,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 31101673,
                    "dataRead": 482495720,
                    "rowCount": 482509,
                    "usageDescription": "",
                    "jobId": 21,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 33:\nresult_df = result_df.join(valuedf,['subreddit','wordhash'],\"right_outer\").cache()\nresult_df.show()",
                    "submissionTime": "2023-11-16T11:25:53.251GMT",
                    "stageIds": [
                      24,
                      25,
                      23
                    ],
                    "jobGroup": "33",
                    "status": "RUNNING",
                    "numTasks": 6161,
                    "numActiveTasks": 9,
                    "numCompletedTasks": 822,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 784,
                    "numActiveStages": 2,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "037a9b2e-190b-40ba-ac91-9555ef701e21"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 33, Cancelled, Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700134863189
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_without_duplicates = joined_df.dropDuplicates()\n",
        "\n",
        "# Show the resulting DataFrame without duplicates\n",
        "result_without_duplicates.cache().show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-11-16T10:45:12.5979973Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-11-16T10:45:12.9206553Z",
              "spark_jobs": null,
              "parent_msg_id": "a5a1a7e7-a23b-407d-baef-9e595c7aaed3"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700131513095
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving intermediate data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intermediate outputs go into the azureml workspace attached storage using the URI `azureml://datastores/workspaceblobstore/paths/<PATH-TO_STORE>` this is the same for all workspaces. Then to re-load you use the same URI"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "joined_df.write.parquet(f\"{CSV_DIR}/sentiment_tfidf.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-11-16T10:45:12.7362793Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-11-16T10:45:12.9215424Z",
              "spark_jobs": null,
              "parent_msg_id": "c9830651-a4e1-4a45-a5b6-3fad91ba3e26"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700131513110
        }
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Comments JSON to Parquet",
      "widgets": {}
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}