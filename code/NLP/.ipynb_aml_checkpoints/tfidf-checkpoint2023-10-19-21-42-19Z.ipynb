{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In interactive notebook, the `spark` object is already created.\n",
        "Instructors tested with 1 driver, 6 executors of small e4 (24 cores, 192GB memory)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch spark environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "51",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:31:28.164828Z",
              "session_start_time": "2023-11-18T22:31:28.2047597Z",
              "execution_start_time": "2023-11-18T22:34:53.5430795Z",
              "execution_finish_time": "2023-11-18T22:34:56.4719635Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "faa80166-04ff-4d9b-84db-eb811795d0d5"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 51, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7fe5629052b0>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-32474553:39627\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.2.5.1-100879434</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700346896289
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f \\\n",
        "{\"conf\": {\"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.2\"}}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:31:28.173993Z",
              "session_start_time": "2023-11-18T22:34:56.5968137Z",
              "execution_start_time": "2023-11-18T22:37:13.0677032Z",
              "execution_finish_time": "2023-11-18T22:37:13.0972721Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "020d075e-cfe1-4332-b8e3-9c76ed3ffb09"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Unrecognized options: "
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up data configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob_account_name = \"marckvnonprodblob\"\n",
        "blob_container_name = \"bigdata\"\n",
        "# read only\n",
        "blob_sas_token = \"?sv=2021-10-04&st=2023-10-04T01%3A42%3A59Z&se=2024-01-02T02%3A42%3A00Z&sr=c&sp=rlf&sig=w3CH9MbCOpwO7DtHlrahc7AlRPxSZZb8MOgS6TaXLzI%3D\"\n",
        "\n",
        "wasbs_base_url = (\n",
        "    f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/\"\n",
        ")\n",
        "spark.conf.set(\n",
        "    f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\",\n",
        "    blob_sas_token,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:31:28.1748639Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:37:46.0383453Z",
              "execution_finish_time": "2023-11-18T22:37:46.3430911Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "f1c3b7df-72c8-441a-aff7-56c550fbc12c"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347066024
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading in single parquet file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_path = \"reddit-parquet/comments/\"\n",
        "submissions_path = \"reddit-parquet/submissions/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:31:28.1759433Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:37:46.4575825Z",
              "execution_finish_time": "2023-11-18T22:37:46.7474673Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "318720bc-573c-4771-b65f-8f1c6ca75383"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347066418
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\"Tetris\",\"pokemon\",\"SuperMario\",\"GTA\",\"CallOfDuty\",\"FIFA\",\"legostarwars\",\n",
        "\"assassinscreed\",\"thesims\",\"FinalFantasy\"] "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:31:28.1770683Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:37:46.8566719Z",
              "execution_finish_time": "2023-11-18T22:37:47.1592246Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "66212ba8-9e06-4394-9666-74ae542d5fa6"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347066818
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reeading in all of the Reddit data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\n",
        "submissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:31:29.3747439Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:37:47.2635705Z",
              "execution_finish_time": "2023-11-18T22:38:26.0879565Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2023-11-18T22:38:19.167GMT",
                    "completionTime": "2023-11-18T22:38:23.785GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2023-11-18T22:38:04.730GMT",
                    "completionTime": "2023-11-18T22:38:09.543GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "1dc0e204-5a72-4ee9-8091-3963efaf3cf6"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347105877
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length, col,split\n",
        "sub_filtered = submissions_df.filter((length(col(\"title\")) > 0)& (col(\"title\") != \"[deleted]\")&(col('title')!= \"[removed]\"))\\\n",
        ".filter(col(\"subreddit\").isin(topic))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:31:34.7583818Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:38:26.1969824Z",
              "execution_finish_time": "2023-11-18T22:38:27.0230159Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "0c2c9178-b24f-48f8-bbcb-768c516f5ee4"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347106691
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_save = sub_filtered.select(\"subreddit\", \"title\",\"year\",\"month\").sample(fraction= 0.2,seed =22).cache()\n",
        "df_save.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:31:38.0713686Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:38:27.1379679Z",
              "execution_finish_time": "2023-11-18T22:38:35.1722433Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 9698162,
                    "rowCount": 221807,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\ndf_save = sub_filtered.select(\"subreddit\", \"title\",\"year\",\"month\").sample(fraction= 0.2,seed =22).cache()\ndf_save.show()",
                    "submissionTime": "2023-11-18T22:38:29.357GMT",
                    "completionTime": "2023-11-18T22:38:34.138GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "5efa1310-46b2-4173-b0d5-6d3923d63e6d"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+----+-----+\n|     subreddit|               title|year|month|\n+--------------+--------------------+----+-----+\n|       pokemon|the PokemonTogeth...|2023|    2|\n|       pokemon|Who's a non-villa...|2023|    2|\n|       thesims|(If we know) when...|2023|    2|\n|    CallOfDuty|SBMM was very nic...|2023|    2|\n|           GTA|Is GTA china town...|2023|    2|\n|assassinscreed|Network issues wi...|2023|    2|\n|       pokemon|Opinion on Pokemo...|2023|    2|\n|       pokemon|     Team Gyarados🥶|2023|    2|\n|       pokemon|I made Tinkaton o...|2023|    2|\n|       pokemon|        Vaporeon...?|2023|    2|\n|  legostarwars|how much is this ...|2023|    2|\n|       pokemon|One of the follow...|2023|    2|\n|          FIFA|89 Ben Yedder or ...|2023|    2|\n|       pokemon|  Pawmo not evolving|2023|    2|\n|       pokemon|Factually the cut...|2023|    2|\n|       pokemon|Main Series games...|2023|    2|\n|          FIFA|Mbappe &amp; Rttf...|2023|    2|\n|           GTA|I used faceapp fo...|2023|    2|\n|  FinalFantasy|Advent Chldren In...|2023|    2|\n|       pokemon|First time EV tra...|2023|    2|\n+--------------+--------------------+----+-----+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347114856
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using TFIDF to identify the key points for each game "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:32:04.4258807Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:38:35.3010342Z",
              "execution_finish_time": "2023-11-18T22:38:51.654807Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "c4b6fca2-7720-4c33-90ea-9a4aa8c6e65d"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting spark-nlp\n  Downloading spark_nlp-5.1.4-py2.py3-none-any.whl (540 kB)\n\u001b[K     |████████████████████████████████| 540 kB 9.6 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: spark-nlp\nSuccessfully installed spark-nlp-5.1.4\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer as tot, StopWordsRemover\n",
        "from pyspark.sql.functions import length, col,split"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:32:14.9604089Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:38:51.7713517Z",
              "execution_finish_time": "2023-11-18T22:38:56.5990119Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "907e8703-5bca-4af4-b727-9be75abebd56"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 13, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347136258
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler() \\\n",
        "    .setInputCol('title') \\\n",
        "    .setOutputCol('document')\n",
        "\n",
        "cleanUpPatterns = [\"<[^>]*>\"]\n",
        "# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\n",
        "documentNormalizer = DocumentNormalizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"normalizedDocument\") \\\n",
        "    .setAction(\"clean\") \\\n",
        "    .setPatterns(cleanUpPatterns) \\\n",
        "    .setReplacement(\" \") \\\n",
        "    .setPolicy(\"pretty_all\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "sentenceDetector = SentenceDetector() \\\n",
        "      .setInputCols([\"normalizedDocument\"]) \\\n",
        "      .setOutputCol(\"sentence\")\n",
        "'''\n",
        "regexTokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"sentence\"]) \\\n",
        "      .setOutputCol(\"token\") \\\n",
        "      .fit(dataeg)\n",
        "'''\n",
        "# tokenization\n",
        "tokenizer = (\n",
        "    Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        ")\n",
        "\n",
        "# make the words back to root form\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "\n",
        "# remove stop words\n",
        "stop_words_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"stem\"])  \\\n",
        "    .setOutputCol(\"cleaned_token\") \\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "# check spelling\n",
        "spellModel = ContextSpellCheckerModel\\\n",
        "    .pretrained(\"spellcheck_dl\", \"en\")\\\n",
        "    .setInputCols(\"cleaned_token\")\\\n",
        "    .setOutputCol(\"final_token\")\\\n",
        "\n",
        "nlpcleanPipeline = \\\n",
        "  Pipeline() \\\n",
        "    .setStages([\n",
        "        documentAssembler,\n",
        "        documentNormalizer,\n",
        "        sentenceDetector,\n",
        "        tokenizer,\n",
        "        stemmer,\n",
        "        stop_words_cleaner,\n",
        "        spellModel])\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:35:11.9649285Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:38:56.7298682Z",
              "execution_finish_time": "2023-11-18T22:40:13.016915Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 10,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 1582672,
                    "rowCount": 2840,
                    "usageDescription": "",
                    "jobId": 12,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:57.287GMT",
                    "completionTime": "2023-11-18T22:39:57.474GMT",
                    "stageIds": [
                      12
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 1253530,
                    "rowCount": 2840,
                    "usageDescription": "",
                    "jobId": 11,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:56.922GMT",
                    "completionTime": "2023-11-18T22:39:57.063GMT",
                    "stageIds": [
                      11
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 1253530,
                    "rowCount": 2840,
                    "usageDescription": "",
                    "jobId": 10,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:56.205GMT",
                    "completionTime": "2023-11-18T22:39:56.406GMT",
                    "stageIds": [
                      10
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:235",
                    "dataWritten": 0,
                    "dataRead": 1364274,
                    "rowCount": 2840,
                    "usageDescription": "",
                    "jobId": 9,
                    "name": "collect at Feature.scala:235",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:55.510GMT",
                    "completionTime": "2023-11-18T22:39:55.888GMT",
                    "stageIds": [
                      9
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at SpecialTokensParser.scala:72",
                    "dataWritten": 0,
                    "dataRead": 36685113,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 8,
                    "name": "collect at SpecialTokensParser.scala:72",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:32.651GMT",
                    "completionTime": "2023-11-18T22:39:54.332GMT",
                    "stageIds": [
                      8
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 9,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 9,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at SpecialTokensParser.scala:72",
                    "dataWritten": 0,
                    "dataRead": 367997,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "collect at SpecialTokensParser.scala:72",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:32.225GMT",
                    "completionTime": "2023-11-18T22:39:32.523GMT",
                    "stageIds": [
                      7
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 9,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 9,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at SpecialTokensParser.scala:72",
                    "dataWritten": 0,
                    "dataRead": 1898328,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "collect at SpecialTokensParser.scala:72",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:31.156GMT",
                    "completionTime": "2023-11-18T22:39:32.128GMT",
                    "stageIds": [
                      6
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 9,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 9,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at SpecialTokensParser.scala:72",
                    "dataWritten": 0,
                    "dataRead": 464508,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 5,
                    "name": "collect at SpecialTokensParser.scala:72",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:30.086GMT",
                    "completionTime": "2023-11-18T22:39:31.024GMT",
                    "stageIds": [
                      5
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 9,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 9,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 9,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "collect at Feature.scala:395",
                    "dataWritten": 0,
                    "dataRead": 511577,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "collect at Feature.scala:395",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:28.682GMT",
                    "completionTime": "2023-11-18T22:39:29.601GMT",
                    "stageIds": [
                      4
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 2,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 2,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 2,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 507,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 14:\ndocumentAssembler = DocumentAssembler()     .setInputCol('title')     .setOutputCol('document')\n\ncleanUpPatterns = [\"<[^>]*>\"]\n# normalizer referred from https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/english/document-normalizer/document_normalizer_notebook.ipynb\ndocumentNormalizer = DocumentNormalizer()     .setInputCols(\"document\")     .setOutputCol(\"normalizedDocument\")     .setAction(\"clean\")     .setPatterns(cleanUpPatterns)     .setReplacement(\" \")     .setPolicy(\"pretty_all\")     .setLowercase(True)\n\nsentenceDetector = SentenceDetector()       .setInputCols([\"normalizedDocument\"])       .setOutputCol(\"sentence\")\n'''\nregexTokenizer = Tokenizer()       .setInputCols([\"sentence\"])       .setOutputCol(\"token\")       .fit(dataeg)\n'''\n# tokenization\ntokenizer = (\n    Tokenizer()     .setInputCols([\"sentence\"])     .setOutputCol(\"token\")\n)\n\n# make the words back to root form\nstemmer = Stemmer()     .setInputCols([\"token\"])     .setOutp...",
                    "submissionTime": "2023-11-18T22:39:28.239GMT",
                    "completionTime": "2023-11-18T22:39:28.525GMT",
                    "stageIds": [
                      3
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "64770998-5fdd-4240-83b6-f90354cb86fa"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "spellcheck_dl download started this may take some time.\nApproximate size to download 95.1 MB\n[OK!]\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347212729
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = nlpcleanPipeline.fit(df_save).transform(df_save)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:35:20.2630476Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:40:13.1239666Z",
              "execution_finish_time": "2023-11-18T22:40:14.752665Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "11fc7a1e-c572-4ed0-8164-7209232fb949"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 15, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347214418
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size, concat_ws\n",
        "new_df_post = cleaned_df.select(\"subreddit\", 'final_token.result')\n",
        "# remove empty array\n",
        "new_df_post_filter = new_df_post.filter(size(col(\"result\")) > 0)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:35:21.811053Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:40:14.8615934Z",
              "execution_finish_time": "2023-11-18T22:40:15.1651107Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9145c2cb-a0e8-41b8-aa19-cc3e4a230cb8"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 16, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347214824
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = new_df_post_filter.withColumn(\"text\", concat_ws(\", \", col(\"result\")))\n",
        "final_df_post = final_df.select(\"subreddit\", 'text')\n",
        "final_df_post.show(10)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T22:35:23.225399Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T22:40:15.2651361Z",
              "execution_finish_time": "2023-11-18T22:40:44.9191957Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 3152,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 13,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 17:\nfinal_df = new_df_post_filter.withColumn(\"text\", concat_ws(\", \", col(\"result\")))\nfinal_df_post = final_df.select(\"subreddit\", 'text')\nfinal_df_post.show(10)",
                    "submissionTime": "2023-11-18T22:40:15.700GMT",
                    "completionTime": "2023-11-18T22:40:43.966GMT",
                    "stageIds": [
                      13
                    ],
                    "jobGroup": "17",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "e6ef16c1-da35-4a26-bc18-5d1f55f0861e"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+\n|     subreddit|                text|\n+--------------+--------------------+\n|       pokemon|pokemontogeth, ca...|\n|       pokemon|who, non-villain,...|\n|       thesims|thank, !, (, know...|\n|    CallOfDuty|same, wa, very, n...|\n|           GTA|Uta, china, town,...|\n|assassinscreed|network, issue, u...|\n|       pokemon|opinion, common, ...|\n|       pokemon|    team, gyarados🥶|\n|       pokemon|Doc, made, Pinkst...|\n|       pokemon|?, ., ., Napoleon, .|\n+--------------+--------------------+\nonly showing top 10 rows\n\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700347244600
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = DocumentAssembler() \\\n",
        ".setInputCol(\"text\") \\\n",
        ".setOutputCol(\"document\")\n",
        "\n",
        "token = Tokenizer() \\\n",
        ".setInputCols([\"document\"]) \\\n",
        ".setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        ".setInputCols([\"token\"]) \\\n",
        ".setOutputCol(\"normal\")\n",
        "\n",
        "vivekn =  ViveknSentimentModel.pretrained() \\\n",
        ".setInputCols([\"document\", \"normal\"]) \\\n",
        ".setOutputCol(\"result_sentiment\")\n",
        "\n",
        "finisher = Finisher() \\\n",
        ".setInputCols([\"result_sentiment\"]) \\\n",
        ".setOutputCols(\"final_sentiment\")\n",
        "\n",
        "pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 51,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T23:19:05.6855596Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T23:19:05.8096096Z",
              "execution_finish_time": "2023-11-18T23:19:13.6323158Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "7b814f06-9f73-4770-ab93-3e9472acf279"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 51, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "sentiment_vivekn download started this may take some time.\nApproximate size to download 873.6 KB\n[OK!]\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700349553337
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipeline.fit(final_df_post).transform(final_df_post).cache()\n",
        "result.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 52,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T23:19:15.7004506Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T23:19:15.8032969Z",
              "execution_finish_time": "2023-11-18T23:19:30.0042035Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 3152,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 35,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 52:\nresult = pipeline.fit(final_df_post).transform(final_df_post).cache()\nresult.show()",
                    "submissionTime": "2023-11-18T23:19:16.346GMT",
                    "completionTime": "2023-11-18T23:19:29.050GMT",
                    "stageIds": [
                      35
                    ],
                    "jobGroup": "52",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "1d4f415d-730f-4a23-86b5-c06dd33fcdeb"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 52, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+---------------+\n|     subreddit|                text|final_sentiment|\n+--------------+--------------------+---------------+\n|       pokemon|pokemontogeth, ca...|     [negative]|\n|       pokemon|who, non-villain,...|     [negative]|\n|       thesims|thank, !, (, know...|     [negative]|\n|    CallOfDuty|same, wa, very, n...|     [positive]|\n|           GTA|Uta, china, town,...|     [negative]|\n|assassinscreed|network, issue, u...|           [na]|\n|       pokemon|opinion, common, ...|     [negative]|\n|       pokemon|    team, gyarados🥶|     [positive]|\n|       pokemon|Doc, made, Pinkst...|     [negative]|\n|       pokemon|?, ., ., Napoleon, .|           [na]|\n|  legostarwars|much, the, coli, ...|     [positive]|\n|       pokemon|follow, common, f...|     [negative]|\n|          FIFA|89, men, leader, ...|     [negative]|\n|       pokemon|         paw, evolve|           [na]|\n|       pokemon|factual, test, co...|     [positive]|\n|       pokemon|main, Geri, game,...|     [positive]|\n|          FIFA|sell, ?, roof, Ph...|     [negative]|\n|           GTA|     us, face, smile|     [negative]|\n|  FinalFantasy|advent, children,...|     [positive]|\n|       pokemon|first, time, ve, ...|     [positive]|\n+--------------+--------------------+---------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700349569653
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, regexp_replace\n",
        "df = result.withColumn(\"text\", regexp_replace(col(\"text\"), \",\", \"\"))\n",
        "df = df.withColumn(\"title\", regexp_replace(col(\"text\"), \".*?([^.]+).*\", \"$1\"))\n",
        "\n",
        "# Remove parentheses from the sentiment\n",
        "df = df.withColumn(\"final_sentiment\", col(\"final_sentiment\")[0])\n",
        "df = df.drop(\"text\")\n",
        "df = df.filter(col(\"final_sentiment\") != \"na\")\n",
        "# Show the resulting DataFrame\n",
        "df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 67,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T23:31:42.6454446Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T23:31:42.7504027Z",
              "execution_finish_time": "2023-11-18T23:31:43.5795651Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 1,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 2560,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 47,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 67:\nfrom pyspark.sql.functions import col, regexp_replace\ndf = result.withColumn(\"text\", regexp_replace(col(\"text\"), \",\", \"\"))\ndf = df.withColumn(\"title\", regexp_replace(col(\"text\"), \".*?([^.]+).*\", \"$1\"))\n\n# Remove parentheses from the sentiment\ndf = df.withColumn(\"final_sentiment\", col(\"final_sentiment\")[0])\ndf = df.drop(\"text\")\ndf = df.filter(col(\"final_sentiment\") != \"na\")\n# Show the resulting DataFrame\ndf.show()",
                    "submissionTime": "2023-11-18T23:31:42.892GMT",
                    "completionTime": "2023-11-18T23:31:43.023GMT",
                    "stageIds": [
                      47
                    ],
                    "jobGroup": "67",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "95a0df39-da42-4117-9909-b4bc8385218b"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 67, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+------------+---------------+--------------------+\n|   subreddit|final_sentiment|               title|\n+------------+---------------+--------------------+\n|     pokemon|       negative|pokemontogeth cam...|\n|     pokemon|       negative|who non-villain r...|\n|     thesims|       negative|thank ! ( know ) ...|\n|  CallOfDuty|       positive|same wa very nice...|\n|         GTA|       negative|Uta china town wa...|\n|     pokemon|       negative|opinion common un...|\n|     pokemon|       positive|     team gyarados🥶|\n|     pokemon|       negative|Doc made Pinkston...|\n|legostarwars|       positive|much the coli Leg...|\n|     pokemon|       negative|follow common fac...|\n|        FIFA|       negative|89 men leader 93 ...|\n|     pokemon|       positive|factual test comm...|\n|     pokemon|       positive|main Geri game ne...|\n|        FIFA|       negative|sell ? roof Phill...|\n|         GTA|       negative|       us face smile|\n|FinalFantasy|       positive|advent children i...|\n|     pokemon|       positive| first time ve train|\n|        FIFA|       negative|ar chance  wish g...|\n|        FIFA|       positive|can’t believe happen|\n|legostarwars|       negative|i'm wonder any ha...|\n+------------+---------------+--------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 64,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700350303227
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 72,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T23:41:51.2607281Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T23:41:51.3648146Z",
              "execution_finish_time": "2023-11-18T23:41:52.410665Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "ccc258ec-06ac-4839-8713-5e2ad5ded6a1"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 72, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 143,
          "data": {
            "text/plain": "1"
          },
          "metadata": {}
        }
      ],
      "execution_count": 69,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700350912081
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(fraction=0.2,seed= 20).cache()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 75,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-18T23:43:35.4759384Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T23:43:35.5709981Z",
              "execution_finish_time": "2023-11-18T23:43:35.8601438Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "43a987a6-2935-4bbd-9d6b-c45507c7309f"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 75, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 72,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700351015514
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "df.write.parquet(f\"{CSV_DIR}/sentiment_tfidf2.parquet\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "52",
              "statement_id": 76,
              "state": "cancelled",
              "livy_statement_state": "waiting",
              "queued_time": "2023-11-18T23:43:57.4782957Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-18T23:43:57.5730839Z",
              "execution_finish_time": "2023-11-18T23:45:58.5507268Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 1,
                  "SUCCEEDED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 53077,
                    "dataRead": 139473731,
                    "rowCount": 3237654,
                    "usageDescription": "",
                    "jobId": 52,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 76:\nimport os\nCSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\ndf.write.parquet(f\"{CSV_DIR}/sentiment_tfidf2.parquet\")",
                    "submissionTime": "2023-11-18T23:43:58.037GMT",
                    "stageIds": [
                      57,
                      58,
                      59
                    ],
                    "jobGroup": "76",
                    "status": "RUNNING",
                    "numTasks": 3086,
                    "numActiveTasks": 9,
                    "numCompletedTasks": 20,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 20,
                    "numActiveStages": 1,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "8cb66360-0025-439d-a733-37e24ada2c71"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 52, 76, Cancelled, Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 73,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700351158030
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.cache()\n",
        "ndf.select(\"subreddit\",\"expwords\",\"year\",\"month\",\"sentiment\",\"wordhash\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:25:19.1803753Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:25:19.2885858Z",
              "execution_finish_time": "2023-11-16T11:25:19.62201Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "80d83379-6a1c-4080-87d4-a4cdb90a796f"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 31, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 61,
          "data": {
            "text/plain": "DataFrame[subreddit: string, expwords: string, year: int, month: int, sentiment: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, wordhash: int]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700133919568
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "udf1 = f.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
        "valuedf = result.select('subreddit',\"filtered_tokens\",\"year\",\"month\",\"sentiment\",f.explode(udf1(f.col('features'))).name('wordhash','value'))\n",
        "valuedf = valuedf.withColumn(\"sentiment\",f.explode(\"sentiment.result\"))\n",
        "valuedf = valuedf.drop_duplicates(subset=[\"wordhash\"])\n",
        "valuedf.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 36,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:43:07.6492253Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:43:07.8041378Z",
              "execution_finish_time": "2023-11-16T11:43:09.3126611Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "371a5faf-db3d-4bd7-bc8d-130b561bccc6"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 36, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---------+--------------------+----+-----+---------+--------+------------------+\n|subreddit|     filtered_tokens|year|month|sentiment|wordhash|             value|\n+---------+--------------------+----+-----+---------+--------+------------------+\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  139265| 9.740140549187382|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   88005| 5.463474430171326|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  215686| 6.332122839048911|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  114628|10.992903517682748|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   63750| 7.041659799101322|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   80646| 20.59951267424561|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   12999|10.992903517682748|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  186058| 5.152261860309351|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  113673|2.5955078897221076|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  152651|  7.76407736196138|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  153551| 19.34229535540086|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   62030| 9.228105951733017|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   31064| 6.471114940633709|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  215514|6.2328686211337425|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  110813|  2.45101771367614|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   98717| 2.479417188197838|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   29214| 6.138922033631871|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  249180|0.5217682340853889|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|    4959|4.4497113554883505|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  123426| 5.801336521081593|\n+---------+--------------------+----+-----+---------+--------+------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700134989254
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valuedf = valuedf.drop_duplicates(subset=[\"subreddit\",\"wordhash\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 38,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:46:10.1396779Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:46:10.2543451Z",
              "execution_finish_time": "2023-11-16T11:46:10.5786422Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "2d88c823-4825-407b-9180-2931739a3ee3"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 38, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700135170488
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valuedf.cache()\n",
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "valuedf.toPandas().to_csv(f\"{CSV_DIR}/analysis-2.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 39,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2023-11-16T11:46:15.2932564Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:46:15.4216838Z",
              "execution_finish_time": null,
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "de45e2eb-810f-4b1e-a631-0099ab02b29a"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 39, Submitted, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700135130854
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.join(valuedf,['subreddit','wordhash'],\"right_outer\").cache()\n",
        "result_df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 33,
              "state": "cancelled",
              "livy_statement_state": "waiting",
              "queued_time": "2023-11-16T11:25:52.5189687Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:25:52.6183166Z",
              "execution_finish_time": "2023-11-16T11:41:03.4114082Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 1,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 31101673,
                    "dataRead": 482495720,
                    "rowCount": 482509,
                    "usageDescription": "",
                    "jobId": 21,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 33:\nresult_df = result_df.join(valuedf,['subreddit','wordhash'],\"right_outer\").cache()\nresult_df.show()",
                    "submissionTime": "2023-11-16T11:25:53.251GMT",
                    "stageIds": [
                      24,
                      25,
                      23
                    ],
                    "jobGroup": "33",
                    "status": "RUNNING",
                    "numTasks": 6161,
                    "numActiveTasks": 9,
                    "numCompletedTasks": 822,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 784,
                    "numActiveStages": 2,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "037a9b2e-190b-40ba-ac91-9555ef701e21"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 33, Cancelled, Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700134863189
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_without_duplicates = joined_df.dropDuplicates()\n",
        "\n",
        "# Show the resulting DataFrame without duplicates\n",
        "result_without_duplicates.cache().show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-11-16T10:45:12.5979973Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-11-16T10:45:12.9206553Z",
              "spark_jobs": null,
              "parent_msg_id": "a5a1a7e7-a23b-407d-baef-9e595c7aaed3"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700131513095
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving intermediate data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intermediate outputs go into the azureml workspace attached storage using the URI `azureml://datastores/workspaceblobstore/paths/<PATH-TO_STORE>` this is the same for all workspaces. Then to re-load you use the same URI"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "joined_df.write.parquet(f\"{CSV_DIR}/sentiment_tfidf.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-11-16T10:45:12.7362793Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-11-16T10:45:12.9215424Z",
              "spark_jobs": null,
              "parent_msg_id": "c9830651-a4e1-4a45-a5b6-3fad91ba3e26"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700131513110
        }
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Comments JSON to Parquet",
      "widgets": {}
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}