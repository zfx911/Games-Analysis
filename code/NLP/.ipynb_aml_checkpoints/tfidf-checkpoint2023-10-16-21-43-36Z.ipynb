{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In interactive notebook, the `spark` object is already created.\n",
        "Instructors tested with 1 driver, 6 executors of small e4 (24 cores, 192GB memory)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch spark environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "36",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7404515Z",
              "session_start_time": "2023-11-16T12:08:21.8037539Z",
              "execution_start_time": "2023-11-16T12:09:25.382156Z",
              "execution_finish_time": "2023-11-16T12:09:25.8035321Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "c8dafb51-f0ae-45b2-8ce6-0768d6fb13c8"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 36, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f2aab50f910>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-86118106:40239\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.2.5.1-100879434</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136565792
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f \\\n",
        "{\"conf\": {\"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.2\"}}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7509121Z",
              "session_start_time": "2023-11-16T12:09:25.9877187Z",
              "execution_start_time": "2023-11-16T12:11:03.6472382Z",
              "execution_finish_time": "2023-11-16T12:11:03.6797834Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "f4bde8a7-5229-410a-8bfb-6a77b9497720"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Unrecognized options: "
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up data configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob_account_name = \"marckvnonprodblob\"\n",
        "blob_container_name = \"bigdata\"\n",
        "# read only\n",
        "blob_sas_token = \"?sv=2021-10-04&st=2023-10-04T01%3A42%3A59Z&se=2024-01-02T02%3A42%3A00Z&sr=c&sp=rlf&sig=w3CH9MbCOpwO7DtHlrahc7AlRPxSZZb8MOgS6TaXLzI%3D\"\n",
        "\n",
        "wasbs_base_url = (\n",
        "    f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/\"\n",
        ")\n",
        "spark.conf.set(\n",
        "    f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\",\n",
        "    blob_sas_token,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7518807Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:11:20.7034335Z",
              "execution_finish_time": "2023-11-16T12:11:21.0043147Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "6964bf02-7b79-4e19-8a05-6e65a0b140f2"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136680898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading in single parquet file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_path = \"reddit-parquet/comments/\"\n",
        "submissions_path = \"reddit-parquet/submissions/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.752753Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:11:21.1273765Z",
              "execution_finish_time": "2023-11-16T12:11:21.4210637Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "825d2763-6274-48eb-8ef4-475f29676563"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136681300
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\"Tetris\",\"pokemon\",\"SuperMario\",\"GTA\",\"CallOfDuty\",\"FIFA\",\"legostarwars\",\n",
        "\"assassinscreed\",\"thesims\",\"FinalFantasy\"] "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.753684Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:11:22.4257895Z",
              "execution_finish_time": "2023-11-16T12:11:22.7336272Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4eeb036e-cd72-43d0-af21-189d77837930"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136682612
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reeading in all of the Reddit data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\n",
        "submissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7547891Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:11:22.8585485Z",
              "execution_finish_time": "2023-11-16T12:13:19.0884034Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 2
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2023-11-16T12:13:12.385GMT",
                    "completionTime": "2023-11-16T12:13:18.134GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2023-11-16T12:11:41.177GMT",
                    "completionTime": "2023-11-16T12:13:02.429GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "a6c06685-2be2-4c9c-ad70-cb1b01c38a64"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136799001
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length, col,split\n",
        "sub_filtered = submissions_df.filter((length(col(\"selftext\")) > 0)& (col(\"selftext\") != \"[deleted]\")&(col('selftext')!= \"[removed]\"))\\\n",
        ".filter(col(\"subreddit\").isin(topic))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7558954Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:13:19.2617292Z",
              "execution_finish_time": "2023-11-16T12:13:20.1008305Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "5637713f-f9c1-4cd1-b4ce-0381c3074c9d"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136800000
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_save = sub_filtered.select(\"subreddit\", \"title\", \"selftext\",\"year\",\"month\").cache()\n",
        "df_save.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7576817Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:13:20.224944Z",
              "execution_finish_time": "2023-11-16T12:13:32.1320197Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 43739239,
                    "rowCount": 221807,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\ndf_save = sub_filtered.select(\"subreddit\", \"title\", \"selftext\",\"year\",\"month\").cache()\ndf_save.show()",
                    "submissionTime": "2023-11-16T12:13:22.383GMT",
                    "completionTime": "2023-11-16T12:13:30.797GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "c930bad6-e423-452c-b20a-2be7f4dffb45"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+--------------------+----+-----+\n|     subreddit|               title|            selftext|year|month|\n+--------------+--------------------+--------------------+----+-----+\n|       pokemon|the PokemonTogeth...|So several days a...|2023|    2|\n|       pokemon|Who's a non-villa...|For me, Tyme insp...|2023|    2|\n|       pokemon|i have a realization|&amp;#x200B;\\n\\n[...|2023|    2|\n|          FIFA|Is there any reas...|For the past 10 d...|2023|    2|\n|           GTA|What should I buy...|I have around 5 m...|2023|    2|\n|           GTA|what is the name ...|I know the Nero i...|2023|    2|\n|       pokemon|Name any Bug type...|Ok now we’re doin...|2023|    2|\n|       pokemon|My starters for e...|Gen 1: Charizard ...|2023|    2|\n|       thesims|The Victoria Chal...|\\n\\nI made my own...|2023|    2|\n|       pokemon|I really fucking ...|I feel like it's ...|2023|    2|\n|       thesims|The sim 4 build m...|So whenever I pla...|2023|    2|\n|          FIFA|  flair passes trait|anyone know which...|2023|    2|\n|  legostarwars|Ghtroc 720 build ...|So I'm going to t...|2023|    2|\n|       pokemon|One of the follow...|I came up with th...|2023|    2|\n|       pokemon|Emi's Daily Pokem...|[Art by me!](http...|2023|    2|\n|assassinscreed|Hot Take: Assassi...|AC Unity has been...|2023|    2|\n|  FinalFantasy|Meme Monday has e...|Meme Monday has e...|2023|    2|\n|       pokemon|First time EV tra...|I'm playing throu...|2023|    2|\n|assassinscreed|I made a long ret...|I used to really ...|2023|    2|\n|       pokemon|monotype run for ...|i have had shield...|2023|    2|\n+--------------+--------------------+--------------------+----+-----+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136812041
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using TFIDF to identify the key points for each game "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7591882Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:13:32.249383Z",
              "execution_finish_time": "2023-11-16T12:13:46.4298462Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9c47f329-b71d-426c-83bf-373e4465011a"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting spark-nlp\n  Downloading spark_nlp-5.1.4-py2.py3-none-any.whl (540 kB)\n\u001b[K     |████████████████████████████████| 540 kB 9.5 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: spark-nlp\nSuccessfully installed spark-nlp-5.1.4\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer as tot, StopWordsRemover\n",
        "from pyspark.sql.functions import length, col,split"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7610458Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:13:46.5642716Z",
              "execution_finish_time": "2023-11-16T12:13:50.0630593Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "445c3d09-d631-4a71-991e-b3768ae88603"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 13, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136829946
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Spark ML components\n",
        "tokenizer_nlp = (\n",
        "    Tokenizer()\n",
        "    .setInputCols([\"document\"])\n",
        "    .setOutputCol(\"tokens_nlp\")\n",
        ")\n",
        "stop_words = (\n",
        "    StopWordsCleaner().pretrained(\"stopwords_iso\",\"en\")\n",
        "    .setInputCols(\"tokens_nlp\")\n",
        "    .setOutputCol(\"cleanTokens\")\n",
        ")\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"selftext\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "\n",
        "sentimental = SentimentDLModel.pretrained(lang=\"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "# Create a pipeline\n",
        "pipeline1 = Pipeline(stages=[documentAssembler, use,sentimental])\n",
        "\n",
        "# Fit the pipeline on the data\n",
        "model = pipeline1.fit(df_save)\n",
        "\n",
        "# Transform the data to get TF-IDF features\n",
        "result = model.transform(df_save)\n",
        "result.cache()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T12:08:21.7625352Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:13:50.1951992Z",
              "execution_finish_time": "2023-11-16T12:15:14.5030424Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 4
                },
                "jobs": [
                  {
                    "displayName": "first at Feature.scala:182",
                    "dataWritten": 0,
                    "dataRead": 355,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 6,
                    "name": "first at Feature.scala:182",
                    "description": "Job group for statement 14:\n# Define the Spark ML components\ntokenizer_nlp = (\n    Tokenizer()\n    .setInputCols([\"document\"])\n    .setOutputCol(\"tokens_nlp\")\n)\nstop_words = (\n    StopWordsCleaner().pretrained(\"stopwords_iso\",\"en\")\n    .setInputCols(\"tokens_nlp\")\n    .setOutputCol(\"cleanTokens\")\n)\n\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"selftext\")    .setOutputCol(\"document\")\n    \nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") .setInputCols([\"document\"]) .setOutputCol(\"sentence_embeddings\")\n\n\nsentimental = SentimentDLModel.pretrained(lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n# Create a pipeline\npipeline1 = Pipeline(stages=[documentAssembler, use,sentimental])\n\n# Fit the pipeline on the data\nmodel = pipeline1.fit(df_save)\n\n# Transform the data to get TF-IDF features\nresult = model.transform(df_save)\nresult.cache()",
                    "submissionTime": "2023-11-16T12:15:06.476GMT",
                    "completionTime": "2023-11-16T12:15:06.741GMT",
                    "stageIds": [
                      6
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 463,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 5,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 14:\n# Define the Spark ML components\ntokenizer_nlp = (\n    Tokenizer()\n    .setInputCols([\"document\"])\n    .setOutputCol(\"tokens_nlp\")\n)\nstop_words = (\n    StopWordsCleaner().pretrained(\"stopwords_iso\",\"en\")\n    .setInputCols(\"tokens_nlp\")\n    .setOutputCol(\"cleanTokens\")\n)\n\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"selftext\")    .setOutputCol(\"document\")\n    \nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") .setInputCols([\"document\"]) .setOutputCol(\"sentence_embeddings\")\n\n\nsentimental = SentimentDLModel.pretrained(lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n# Create a pipeline\npipeline1 = Pipeline(stages=[documentAssembler, use,sentimental])\n\n# Fit the pipeline on the data\nmodel = pipeline1.fit(df_save)\n\n# Transform the data to get TF-IDF features\nresult = model.transform(df_save)\nresult.cache()",
                    "submissionTime": "2023-11-16T12:15:06.055GMT",
                    "completionTime": "2023-11-16T12:15:06.314GMT",
                    "stageIds": [
                      5
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 346,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 4,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 14:\n# Define the Spark ML components\ntokenizer_nlp = (\n    Tokenizer()\n    .setInputCols([\"document\"])\n    .setOutputCol(\"tokens_nlp\")\n)\nstop_words = (\n    StopWordsCleaner().pretrained(\"stopwords_iso\",\"en\")\n    .setInputCols(\"tokens_nlp\")\n    .setOutputCol(\"cleanTokens\")\n)\n\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"selftext\")    .setOutputCol(\"document\")\n    \nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") .setInputCols([\"document\"]) .setOutputCol(\"sentence_embeddings\")\n\n\nsentimental = SentimentDLModel.pretrained(lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n# Create a pipeline\npipeline1 = Pipeline(stages=[documentAssembler, use,sentimental])\n\n# Fit the pipeline on the data\nmodel = pipeline1.fit(df_save)\n\n# Transform the data to get TF-IDF features\nresult = model.transform(df_save)\nresult.cache()",
                    "submissionTime": "2023-11-16T12:14:02.340GMT",
                    "completionTime": "2023-11-16T12:14:02.897GMT",
                    "stageIds": [
                      4
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "first at ReadWrite.scala:587",
                    "dataWritten": 0,
                    "dataRead": 4172,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 3,
                    "name": "first at ReadWrite.scala:587",
                    "description": "Job group for statement 14:\n# Define the Spark ML components\ntokenizer_nlp = (\n    Tokenizer()\n    .setInputCols([\"document\"])\n    .setOutputCol(\"tokens_nlp\")\n)\nstop_words = (\n    StopWordsCleaner().pretrained(\"stopwords_iso\",\"en\")\n    .setInputCols(\"tokens_nlp\")\n    .setOutputCol(\"cleanTokens\")\n)\n\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"selftext\")    .setOutputCol(\"document\")\n    \nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") .setInputCols([\"document\"]) .setOutputCol(\"sentence_embeddings\")\n\n\nsentimental = SentimentDLModel.pretrained(lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n# Create a pipeline\npipeline1 = Pipeline(stages=[documentAssembler, use,sentimental])\n\n# Fit the pipeline on the data\nmodel = pipeline1.fit(df_save)\n\n# Transform the data to get TF-IDF features\nresult = model.transform(df_save)\nresult.cache()",
                    "submissionTime": "2023-11-16T12:13:56.456GMT",
                    "completionTime": "2023-11-16T12:13:56.834GMT",
                    "stageIds": [
                      3
                    ],
                    "jobGroup": "14",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "60088390-14d5-40c6-8ccb-29b819eb9e95"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "stopwords_iso download started this may take some time.\nApproximate size to download 2.1 KB\n[OK!]\ntfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n[OK!]\nsentimentdl_use_imdb download started this may take some time.\nApproximate size to download 12 MB\n[OK!]\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "DataFrame[subreddit: string, title: string, selftext: string, year: int, month: int, document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, sentence_embeddings: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, sentiment: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136914505
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tot(inputCol=\"selftext\", outputCol=\"tokens\")\n",
        "\n",
        "# StopWordsRemover\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "\n",
        "# HashingTF and IDF\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# Pipeline\n",
        "pipeline2 = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf])\n",
        "\n",
        "# Fit and transform the data\n",
        "model = pipeline2.fit(result)\n",
        "result = model.transform(result)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "37",
              "statement_id": 15,
              "state": "cancelled",
              "livy_statement_state": "waiting",
              "queued_time": "2023-11-16T12:08:21.7641645Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T12:15:14.6369624Z",
              "execution_finish_time": "2023-11-16T12:15:34.4222849Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 1,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [
                  {
                    "displayName": "treeAggregate at IDF.scala:55",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 7,
                    "name": "treeAggregate at IDF.scala:55",
                    "description": "Job group for statement 15:\ntokenizer = tot(inputCol=\"selftext\", outputCol=\"tokens\")\n\n# StopWordsRemover\nstopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n\n# HashingTF and IDF\nhashing_tf = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n\n# Pipeline\npipeline2 = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, idf])\n\n# Fit and transform the data\nmodel = pipeline2.fit(result)\nresult = model.transform(result)\n\n",
                    "submissionTime": "2023-11-16T12:15:15.849GMT",
                    "stageIds": [
                      7,
                      8
                    ],
                    "jobGroup": "15",
                    "status": "RUNNING",
                    "numTasks": 3135,
                    "numActiveTasks": 8,
                    "numCompletedTasks": 0,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 0,
                    "numActiveStages": 1,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "57b0d36e-5719-44ef-8c93-d7b3db23b21d"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 37, 15, Cancelled, Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136934177
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.sample(fraction=0.2, seed=22)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2023-11-16T12:08:21.7674832Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "spark_jobs": null,
              "parent_msg_id": "8305e2b9-0842-4383-931d-85cbb5f5ff3a"
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700136487132
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.types import MapType, StringType,ArrayType, IntegerType\n",
        "ndf = result.select(\"subreddit\",f.explode('filtered_tokens').name('expwords'),\"rawFeatures\",\"year\",\"month\",\"sentiment\").withColumn('filtered_tokens',f.array('expwords'))\n",
        "hashudf = f.udf(lambda vector: vector.indices.tolist(),ArrayType(IntegerType()))\n",
        "ndf = ndf.drop_duplicates(subset= [\"expwords\"])\n",
        "ndf = ndf.withColumn(\"row_index\", f.monotonically_increasing_id())\n",
        "ndf.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2023-11-16T12:08:21.7693406Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null,
              "spark_jobs": null,
              "parent_msg_id": "bfd0d2c0-98b6-4798-837b-518ceafe189b"
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700133636631
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#wordtf = result.select(\"rawFeatures\").withColumn('wordhash', hashudf(col('rawFeatures')))\n",
        "#wordtf = wordtf.withColumn(\"row_index\", f.monotonically_increasing_id())\n",
        "#wordtf = wordtf.withColumn('wordhash',f.explode(\"wordhash\"))\n",
        "#wordtf =wordtf.drop_duplicates(subset=[\"wordhash\"])\n",
        "#wordtf.cache()\n",
        "# Add the exploded column directly to 'ndf' using crossJoin\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:20:41.5571002Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:20:41.6910149Z",
              "execution_finish_time": "2023-11-16T11:20:42.0033154Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "2c3274cc-eec4-401d-a404-85788631d8ea"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 27, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": "DataFrame[rawFeatures: vector, wordhash: int, row_index: bigint]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700133641948
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndf.cache()\n",
        "ndf.select(\"subreddit\",\"expwords\",\"year\",\"month\",\"sentiment\",\"wordhash\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 31,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:25:19.1803753Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:25:19.2885858Z",
              "execution_finish_time": "2023-11-16T11:25:19.62201Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "80d83379-6a1c-4080-87d4-a4cdb90a796f"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 31, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 61,
          "data": {
            "text/plain": "DataFrame[subreddit: string, expwords: string, year: int, month: int, sentiment: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, wordhash: int]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700133919568
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "udf1 = f.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
        "valuedf = result.select('subreddit',\"filtered_tokens\",\"year\",\"month\",\"sentiment\",f.explode(udf1(f.col('features'))).name('wordhash','value'))\n",
        "valuedf = valuedf.withColumn(\"sentiment\",f.explode(\"sentiment.result\"))\n",
        "valuedf = valuedf.drop_duplicates(subset=[\"wordhash\"])\n",
        "valuedf.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 36,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:43:07.6492253Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:43:07.8041378Z",
              "execution_finish_time": "2023-11-16T11:43:09.3126611Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "371a5faf-db3d-4bd7-bc8d-130b561bccc6"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 36, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---------+--------------------+----+-----+---------+--------+------------------+\n|subreddit|     filtered_tokens|year|month|sentiment|wordhash|             value|\n+---------+--------------------+----+-----+---------+--------+------------------+\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  139265| 9.740140549187382|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   88005| 5.463474430171326|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  215686| 6.332122839048911|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  114628|10.992903517682748|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   63750| 7.041659799101322|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   80646| 20.59951267424561|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   12999|10.992903517682748|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  186058| 5.152261860309351|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  113673|2.5955078897221076|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  152651|  7.76407736196138|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  153551| 19.34229535540086|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   62030| 9.228105951733017|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   31064| 6.471114940633709|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  215514|6.2328686211337425|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  110813|  2.45101771367614|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   98717| 2.479417188197838|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|   29214| 6.138922033631871|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  249180|0.5217682340853889|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|    4959|4.4497113554883505|\n|  pokemon|[gen, 1:, chariza...|2023|    2|      pos|  123426| 5.801336521081593|\n+---------+--------------------+----+-----+---------+--------+------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700134989254
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valuedf = valuedf.drop_duplicates(subset=[\"subreddit\",\"wordhash\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 38,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T11:46:10.1396779Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:46:10.2543451Z",
              "execution_finish_time": "2023-11-16T11:46:10.5786422Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "2d88c823-4825-407b-9180-2931739a3ee3"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 38, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 49,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700135170488
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valuedf.cache()\n",
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "valuedf.toPandas().to_csv(f\"{CSV_DIR}/analysis-2.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 39,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2023-11-16T11:46:15.2932564Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:46:15.4216838Z",
              "execution_finish_time": null,
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "de45e2eb-810f-4b1e-a631-0099ab02b29a"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 39, Submitted, Running)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700135130854
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = result_df.join(valuedf,['subreddit','wordhash'],\"right_outer\").cache()\n",
        "result_df.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "34",
              "statement_id": 33,
              "state": "cancelled",
              "livy_statement_state": "waiting",
              "queued_time": "2023-11-16T11:25:52.5189687Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T11:25:52.6183166Z",
              "execution_finish_time": "2023-11-16T11:41:03.4114082Z",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "RUNNING": 1,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 31101673,
                    "dataRead": 482495720,
                    "rowCount": 482509,
                    "usageDescription": "",
                    "jobId": 21,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 33:\nresult_df = result_df.join(valuedf,['subreddit','wordhash'],\"right_outer\").cache()\nresult_df.show()",
                    "submissionTime": "2023-11-16T11:25:53.251GMT",
                    "stageIds": [
                      24,
                      25,
                      23
                    ],
                    "jobGroup": "33",
                    "status": "RUNNING",
                    "numTasks": 6161,
                    "numActiveTasks": 9,
                    "numCompletedTasks": 822,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 784,
                    "numActiveStages": 2,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "037a9b2e-190b-40ba-ac91-9555ef701e21"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 34, 33, Cancelled, Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700134863189
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_without_duplicates = joined_df.dropDuplicates()\n",
        "\n",
        "# Show the resulting DataFrame without duplicates\n",
        "result_without_duplicates.cache().show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-11-16T10:45:12.5979973Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-11-16T10:45:12.9206553Z",
              "spark_jobs": null,
              "parent_msg_id": "a5a1a7e7-a23b-407d-baef-9e595c7aaed3"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700131513095
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving intermediate data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intermediate outputs go into the azureml workspace attached storage using the URI `azureml://datastores/workspaceblobstore/paths/<PATH-TO_STORE>` this is the same for all workspaces. Then to re-load you use the same URI"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "joined_df.write.parquet(f\"{CSV_DIR}/sentiment_tfidf.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "cancelled",
              "livy_statement_state": null,
              "queued_time": "2023-11-16T10:45:12.7362793Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": "2023-11-16T10:45:12.9215424Z",
              "spark_jobs": null,
              "parent_msg_id": "c9830651-a4e1-4a45-a5b6-3fad91ba3e26"
            },
            "text/plain": "StatementMeta(, , , Cancelled, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700131513110
        }
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Comments JSON to Parquet",
      "widgets": {}
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}