{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In interactive notebook, the `spark` object is already created.\n",
        "Instructors tested with 1 driver, 6 executors of small e4 (24 cores, 192GB memory)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch spark environment"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "17",
              "statement_id": 37,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:38:18.2986562Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T04:38:18.4178331Z",
              "execution_finish_time": "2023-11-16T04:38:18.7163266Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "3d134a0b-1947-4357-881c-7bf1900c2023"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 17, 37, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 73,
          "data": {
            "text/plain": "<pyspark.sql.session.SparkSession at 0x7f78beb62910>",
            "text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://vm-24059505:43055\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.2.5.1-100879434</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Azure ML Experiment</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700109814823
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%configure -f \\\n",
        "{\"conf\": {\"spark.jars.packages\": \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.2\"}}"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": -1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:43:40.9201287Z",
              "session_start_time": "2023-11-16T04:43:40.9712369Z",
              "execution_start_time": "2023-11-16T04:45:08.423313Z",
              "execution_finish_time": "2023-11-16T04:45:08.4524809Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "aef77dea-65b5-4731-b3d9-a267fc1e539c"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, -1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Unrecognized options: "
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up data configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob_account_name = \"marckvnonprodblob\"\n",
        "blob_container_name = \"bigdata\"\n",
        "# read only\n",
        "blob_sas_token = \"?sv=2021-10-04&st=2023-10-04T01%3A42%3A59Z&se=2024-01-02T02%3A42%3A00Z&sr=c&sp=rlf&sig=w3CH9MbCOpwO7DtHlrahc7AlRPxSZZb8MOgS6TaXLzI%3D\"\n",
        "\n",
        "wasbs_base_url = (\n",
        "    f\"wasbs://{blob_container_name}@{blob_account_name}.blob.core.windows.net/\"\n",
        ")\n",
        "spark.conf.set(\n",
        "    f\"fs.azure.sas.{blob_container_name}.{blob_account_name}.blob.core.windows.net\",\n",
        "    blob_sas_token,\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:44:49.9703303Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T04:45:26.8595759Z",
              "execution_finish_time": "2023-11-16T04:45:27.1504526Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d8461d3c-82d2-49e0-a612-3d85b6c275b7"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700109925973
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reading in single parquet file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_path = \"reddit-parquet/comments/\"\n",
        "submissions_path = \"reddit-parquet/submissions/\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:44:51.6793168Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T04:45:27.2624877Z",
              "execution_finish_time": "2023-11-16T04:45:27.5492668Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "baeb9c54-ed40-4162-a2e1-bc93bd5f63c7"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700109926334
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\"Tetris\",\"pokemon\",\"SuperMario\",\"GTA\",\"CallOfDuty\",\"FIFA\",\"legostarwars\",\n",
        "\"assassinscreed\",\"thesims\",\"FinalFantasy\"] "
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:44:52.3566696Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T04:45:27.6522379Z",
              "execution_finish_time": "2023-11-16T04:45:28.0470169Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9eeabe9e-acb6-4ac8-8601-20be37b1f01d"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700109926831
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reeading in all of the Reddit data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\n",
        "submissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:44:54.2931321Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T04:45:28.1890793Z",
              "execution_finish_time": "2023-11-16T04:45:58.3452047Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 2,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2023-11-16T04:45:57.681GMT",
                    "completionTime": "2023-11-16T04:45:58.021GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 0,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 9:\ncomments_df = spark.read.parquet(f\"{wasbs_base_url}{comments_path}\")\nsubmissions_df = spark.read.parquet(f\"{wasbs_base_url}{submissions_path}\")",
                    "submissionTime": "2023-11-16T04:45:46.456GMT",
                    "completionTime": "2023-11-16T04:45:49.694GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "9",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "127e7dce-3d68-4377-b1f2-d63ac7d604a4"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700109957118
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length, col,split\n",
        "sub_filtered = submissions_df.filter((length(col(\"selftext\")) > 0)& (col(\"selftext\") != \"[deleted]\")&(col('selftext')!= \"[removed]\"))\\\n",
        ".filter(col(\"subreddit\").isin(topic))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:44:55.3975214Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T04:45:58.4659855Z",
              "execution_finish_time": "2023-11-16T04:45:59.2488058Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "d3c174ab-56f9-4970-b68a-116ab980dd2f"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700109958002
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_save = sub_filtered.select(\"subreddit\", \"title\", \"selftext\",\"year\",\"month\").cache()\n",
        "df_save.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:44:56.1307098Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T04:45:59.3515617Z",
              "execution_finish_time": "2023-11-16T04:46:11.2857684Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 43739239,
                    "rowCount": 221807,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\ndf_save = sub_filtered.select(\"subreddit\", \"title\", \"selftext\",\"year\",\"month\").cache()\ndf_save.show()",
                    "submissionTime": "2023-11-16T04:46:01.347GMT",
                    "completionTime": "2023-11-16T04:46:09.746GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9ae5b654-8595-4821-97a7-9af01f25835d"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+--------------------+----+-----+\n|     subreddit|               title|            selftext|year|month|\n+--------------+--------------------+--------------------+----+-----+\n|       pokemon|the PokemonTogeth...|So several days a...|2023|    2|\n|       pokemon|Who's a non-villa...|For me, Tyme insp...|2023|    2|\n|       pokemon|i have a realization|&amp;#x200B;\\n\\n[...|2023|    2|\n|          FIFA|Is there any reas...|For the past 10 d...|2023|    2|\n|           GTA|What should I buy...|I have around 5 m...|2023|    2|\n|           GTA|what is the name ...|I know the Nero i...|2023|    2|\n|       pokemon|Name any Bug type...|Ok now we’re doin...|2023|    2|\n|       pokemon|My starters for e...|Gen 1: Charizard ...|2023|    2|\n|       thesims|The Victoria Chal...|\\n\\nI made my own...|2023|    2|\n|       pokemon|I really fucking ...|I feel like it's ...|2023|    2|\n|       thesims|The sim 4 build m...|So whenever I pla...|2023|    2|\n|          FIFA|  flair passes trait|anyone know which...|2023|    2|\n|  legostarwars|Ghtroc 720 build ...|So I'm going to t...|2023|    2|\n|       pokemon|One of the follow...|I came up with th...|2023|    2|\n|       pokemon|Emi's Daily Pokem...|[Art by me!](http...|2023|    2|\n|assassinscreed|Hot Take: Assassi...|AC Unity has been...|2023|    2|\n|  FinalFantasy|Meme Monday has e...|Meme Monday has e...|2023|    2|\n|       pokemon|First time EV tra...|I'm playing throu...|2023|    2|\n|assassinscreed|I made a long ret...|I used to really ...|2023|    2|\n|       pokemon|monotype run for ...|i have had shield...|2023|    2|\n+--------------+--------------------+--------------------+----+-----+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700109970048
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using TFIDF to identify the key points for each game "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spark-nlp"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T04:44:57.5513075Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T04:46:11.3885112Z",
              "execution_finish_time": "2023-11-16T04:46:17.6504154Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "dca7313c-532b-44c8-99c4-ea9ac2e9e6f2"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: spark-nlp in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (5.1.4)\r\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer as tot, StopWordsRemover"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 32,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T05:09:12.5944929Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T05:09:12.7040097Z",
              "execution_finish_time": "2023-11-16T05:09:13.0119824Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "45062981-fed5-4279-b577-609441ae1124"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 32, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700111351797
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Spark ML components\n",
        "tokenizer_nlp = (\n",
        "    Tokenizer()\n",
        "    .setInputCols([\"document\"])\n",
        "    .setOutputCol(\"tokens_nlp\")\n",
        ")\n",
        "stop_words = (\n",
        "    StopWordsCleaner().pretrained(\"stopwords_iso\",\"en\")\n",
        "    .setInputCols(\"tokens_nlp\")\n",
        "    .setOutputCol(\"cleanTokens\")\n",
        ")\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"selftext\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "    \n",
        "use = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\")\\\n",
        " .setInputCols([\"document\"])\\\n",
        " .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "\n",
        "sentimental = SentimentDLModel.pretrained(lang=\"en\")\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"sentiment\")\n",
        "# Create a pipeline\n",
        "pipeline1 = Pipeline(stages=[documentAssembler, use,sentimental])\n",
        "\n",
        "# Fit the pipeline on the data\n",
        "model = pipeline1.fit(df_save)\n",
        "\n",
        "# Transform the data to get TF-IDF features\n",
        "result = model.transform(df_save)\n",
        "result.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 97,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T06:35:08.8626087Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T06:35:08.9597896Z",
              "execution_finish_time": "2023-11-16T06:35:21.0696744Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 32184,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 53,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 97:\n# Define the Spark ML components\ntokenizer_nlp = (\n    Tokenizer()\n    .setInputCols([\"document\"])\n    .setOutputCol(\"tokens_nlp\")\n)\nstop_words = (\n    StopWordsCleaner().pretrained(\"stopwords_iso\",\"en\")\n    .setInputCols(\"tokens_nlp\")\n    .setOutputCol(\"cleanTokens\")\n)\n\ndocumentAssembler = DocumentAssembler()    .setInputCol(\"selftext\")    .setOutputCol(\"document\")\n    \nuse = UniversalSentenceEncoder.pretrained(name=\"tfhub_use\", lang=\"en\") .setInputCols([\"document\"]) .setOutputCol(\"sentence_embeddings\")\n\n\nsentimental = SentimentDLModel.pretrained(lang=\"en\")    .setInputCols([\"sentence_embeddings\"])    .setOutputCol(\"sentiment\")\n# Create a pipeline\npipeline1 = Pipeline(stages=[documentAssembler, use,sentimental])\n\n# Fit the pipeline on the data\nmodel = pipeline1.fit(df_save)\n\n# Transform the data to get TF-IDF features\nresult = model.transform(df_save)\nresult.show()",
                    "submissionTime": "2023-11-16T06:35:19.565GMT",
                    "completionTime": "2023-11-16T06:35:19.702GMT",
                    "stageIds": [
                      57
                    ],
                    "jobGroup": "97",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "5d108b2f-d40a-4c06-ac6a-3752c6c32799"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 97, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "stopwords_iso download started this may take some time.\nApproximate size to download 2.1 KB\n[OK!]\ntfhub_use download started this may take some time.\nApproximate size to download 923.7 MB\n[OK!]\nsentimentdl_use_imdb download started this may take some time.\nApproximate size to download 12 MB\n[OK!]\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+\n|     subreddit|               title|            selftext|year|month|            document| sentence_embeddings|           sentiment|\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+\n|       pokemon|the PokemonTogeth...|So several days a...|2023|    2|[{document, 0, 13...|[{sentence_embedd...|[{category, 0, 13...|\n|       pokemon|Who's a non-villa...|For me, Tyme insp...|2023|    2|[{document, 0, 66...|[{sentence_embedd...|[{category, 0, 66...|\n|       pokemon|i have a realization|&amp;#x200B;\\n\\n[...|2023|    2|[{document, 0, 18...|[{sentence_embedd...|[{category, 0, 18...|\n|          FIFA|Is there any reas...|For the past 10 d...|2023|    2|[{document, 0, 63...|[{sentence_embedd...|[{category, 0, 63...|\n|           GTA|What should I buy...|I have around 5 m...|2023|    2|[{document, 0, 22...|[{sentence_embedd...|[{category, 0, 22...|\n|           GTA|what is the name ...|I know the Nero i...|2023|    2|[{document, 0, 95...|[{sentence_embedd...|[{category, 0, 95...|\n|       pokemon|Name any Bug type...|Ok now we’re doin...|2023|    2|[{document, 0, 42...|[{sentence_embedd...|[{category, 0, 42...|\n|       pokemon|My starters for e...|Gen 1: Charizard ...|2023|    2|[{document, 0, 30...|[{sentence_embedd...|[{category, 0, 30...|\n|       thesims|The Victoria Chal...|\\n\\nI made my own...|2023|    2|[{document, 0, 43...|[{sentence_embedd...|[{category, 0, 43...|\n|       pokemon|I really fucking ...|I feel like it's ...|2023|    2|[{document, 0, 37...|[{sentence_embedd...|[{category, 0, 37...|\n|       thesims|The sim 4 build m...|So whenever I pla...|2023|    2|[{document, 0, 33...|[{sentence_embedd...|[{category, 0, 33...|\n|          FIFA|  flair passes trait|anyone know which...|2023|    2|[{document, 0, 13...|[{sentence_embedd...|[{category, 0, 13...|\n|  legostarwars|Ghtroc 720 build ...|So I'm going to t...|2023|    2|[{document, 0, 41...|[{sentence_embedd...|[{category, 0, 41...|\n|       pokemon|One of the follow...|I came up with th...|2023|    2|[{document, 0, 60...|[{sentence_embedd...|[{category, 0, 60...|\n|       pokemon|Emi's Daily Pokem...|[Art by me!](http...|2023|    2|[{document, 0, 39...|[{sentence_embedd...|[{category, 0, 39...|\n|assassinscreed|Hot Take: Assassi...|AC Unity has been...|2023|    2|[{document, 0, 22...|[{sentence_embedd...|[{category, 0, 22...|\n|  FinalFantasy|Meme Monday has e...|Meme Monday has e...|2023|    2|[{document, 0, 14...|[{sentence_embedd...|[{category, 0, 14...|\n|       pokemon|First time EV tra...|I'm playing throu...|2023|    2|[{document, 0, 41...|[{sentence_embedd...|[{category, 0, 41...|\n|assassinscreed|I made a long ret...|I used to really ...|2023|    2|[{document, 0, 16...|[{sentence_embedd...|[{category, 0, 16...|\n|       pokemon|monotype run for ...|i have had shield...|2023|    2|[{document, 0, 27...|[{sentence_embedd...|[{category, 0, 27...|\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 93,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700116521300
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the HashingTF stage\n",
        "tokenizer = tot(inputCol=\"selftext\", outputCol=\"tokens\")\n",
        "stopwords_remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "exploder = stopwords_remover.transform(tokenizer.transform(result)).select(\"selftext\", F.explode(\"filtered_tokens\").alias(\"filtered_tokens\"))\n",
        "hashing_tf = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "pipeline2 = Pipeline(stages=[tokenizer, stopwords_remover])\n",
        "model = pipeline2.fit(result)\n",
        "\n",
        "# Transform the data to get TF-IDF features\n",
        "result2 = model.transform(result)\n",
        "result2 = result2.withColumn(\"filtered_tokens\",f.explode(\"filtered_tokens\"))\n",
        "pipeline3 = Pipeline(stages=[hashing_tf,idf])\n",
        "model2 = pipeline3.fit(result2)\n",
        "result3 = model2.transform(result2)\n",
        "result3.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 98,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T06:35:26.5097596Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T06:35:26.6064725Z",
              "execution_finish_time": "2023-11-16T06:35:27.4302266Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "afb92f21-c385-4c14-bad6-9ca1f93232a0"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 98, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "requirement failed: The input column must be array, but got string.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_17302/911647287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtered_tokens\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtered_tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpipeline3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhashing_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mresult3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresult3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: The input column must be array, but got string."
          ]
        }
      ],
      "execution_count": 94,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700116527509
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.cache().show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 37,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T05:31:13.0758468Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T05:31:13.1802325Z",
              "execution_finish_time": "2023-11-16T05:31:13.989799Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 381392,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 15,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 37:\nresult.cache().show()",
                    "submissionTime": "2023-11-16T05:31:13.269GMT",
                    "completionTime": "2023-11-16T05:31:13.410GMT",
                    "stageIds": [
                      18
                    ],
                    "jobGroup": "37",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "72432319-38ad-4308-aa2a-4210f4ac69aa"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 37, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|     subreddit|               title|            selftext|year|month|            document| sentence_embeddings|           sentiment|              tokens|     filtered_tokens|         rawFeatures|            features|\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|       pokemon|the PokemonTogeth...|So several days a...|2023|    2|[{document, 0, 13...|[{sentence_embedd...|[{category, 0, 13...|[so, several, day...|[several, days, a...|(262144,[3888,840...|(262144,[3888,840...|\n|       pokemon|Who's a non-villa...|For me, Tyme insp...|2023|    2|[{document, 0, 66...|[{sentence_embedd...|[{category, 0, 66...|[for, me,, tyme, ...|[me,, tyme, inspi...|(262144,[3421,100...|(262144,[3421,100...|\n|       pokemon|i have a realization|&amp;#x200B;\\n\\n[...|2023|    2|[{document, 0, 18...|[{sentence_embedd...|[{category, 0, 18...|[&amp;#x200b;, , ...|[&amp;#x200b;, , ...|(262144,[58503,13...|(262144,[58503,13...|\n|          FIFA|Is there any reas...|For the past 10 d...|2023|    2|[{document, 0, 63...|[{sentence_embedd...|[{category, 0, 63...|[for, the, past, ...|[past, 10, days, ...|(262144,[1889,230...|(262144,[1889,230...|\n|           GTA|What should I buy...|I have around 5 m...|2023|    2|[{document, 0, 22...|[{sentence_embedd...|[{category, 0, 22...|[i, have, around,...|[around, 5, mil, ...|(262144,[18697,28...|(262144,[18697,28...|\n|           GTA|what is the name ...|I know the Nero i...|2023|    2|[{document, 0, 95...|[{sentence_embedd...|[{category, 0, 95...|[i, know, the, ne...|[know, nero, one,...|(262144,[21823,87...|(262144,[21823,87...|\n|       pokemon|Name any Bug type...|Ok now we’re doin...|2023|    2|[{document, 0, 42...|[{sentence_embedd...|[{category, 0, 42...|[ok, now, we’re, ...|[ok, we’re, worst...|(262144,[10564,39...|(262144,[10564,39...|\n|       pokemon|My starters for e...|Gen 1: Charizard ...|2023|    2|[{document, 0, 30...|[{sentence_embedd...|[{category, 0, 30...|[gen, 1:, chariza...|[gen, 1:, chariza...|(262144,[4959,129...|(262144,[4959,129...|\n|       thesims|The Victoria Chal...|\\n\\nI made my own...|2023|    2|[{document, 0, 43...|[{sentence_embedd...|[{category, 0, 43...|[, , i, made, my,...|[, , made, challe...|(262144,[3436,353...|(262144,[3436,353...|\n|       pokemon|I really fucking ...|I feel like it's ...|2023|    2|[{document, 0, 37...|[{sentence_embedd...|[{category, 0, 37...|[i, feel, like, i...|[feel, like, fuck...|(262144,[8254,208...|(262144,[8254,208...|\n|       thesims|The sim 4 build m...|So whenever I pla...|2023|    2|[{document, 0, 33...|[{sentence_embedd...|[{category, 0, 33...|[so, whenever, i,...|[whenever, play, ...|(262144,[3530,157...|(262144,[3530,157...|\n|          FIFA|  flair passes trait|anyone know which...|2023|    2|[{document, 0, 13...|[{sentence_embedd...|[{category, 0, 13...|[anyone, know, wh...|[anyone, know, pl...|(262144,[21823,28...|(262144,[21823,28...|\n|  legostarwars|Ghtroc 720 build ...|So I'm going to t...|2023|    2|[{document, 0, 41...|[{sentence_embedd...|[{category, 0, 41...|[so, i'm, going, ...|[going, try, make...|(262144,[6821,853...|(262144,[6821,853...|\n|       pokemon|One of the follow...|I came up with th...|2023|    2|[{document, 0, 60...|[{sentence_embedd...|[{category, 0, 60...|[i, came, up, wit...|[came, poll, walk...|(262144,[3184,443...|(262144,[3184,443...|\n|       pokemon|Emi's Daily Pokem...|[Art by me!](http...|2023|    2|[{document, 0, 39...|[{sentence_embedd...|[{category, 0, 39...|[[art, by, me!](h...|[[art, me!](https...|(262144,[980,1578...|(262144,[980,1578...|\n|assassinscreed|Hot Take: Assassi...|AC Unity has been...|2023|    2|[{document, 0, 22...|[{sentence_embedd...|[{category, 0, 22...|[ac, unity, has, ...|[ac, unity, getti...|(262144,[471,1376...|(262144,[471,1376...|\n|  FinalFantasy|Meme Monday has e...|Meme Monday has e...|2023|    2|[{document, 0, 14...|[{sentence_embedd...|[{category, 0, 14...|[meme, monday, ha...|[meme, monday, en...|(262144,[32060,36...|(262144,[32060,36...|\n|       pokemon|First time EV tra...|I'm playing throu...|2023|    2|[{document, 0, 41...|[{sentence_embedd...|[{category, 0, 41...|[i'm, playing, th...|[playing, scarlet...|(262144,[21823,26...|(262144,[21823,26...|\n|assassinscreed|I made a long ret...|I used to really ...|2023|    2|[{document, 0, 16...|[{sentence_embedd...|[{category, 0, 16...|[i, used, to, rea...|[used, really, di...|(262144,[2120,347...|(262144,[2120,347...|\n|       pokemon|monotype run for ...|i have had shield...|2023|    2|[{document, 0, 27...|[{sentence_embedd...|[{category, 0, 27...|[i, have, had, sh...|[shield, 2, years...|(262144,[12524,15...|(262144,[12524,15...|\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700112674133
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = result.withColumn(\"sentiment\",F.explode('sentiment.result'))\n",
        "result.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 39,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T05:31:28.9433672Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T05:31:29.0406952Z",
              "execution_finish_time": "2023-11-16T05:31:30.5189793Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 381392,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 16,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 39:\nresult = result.withColumn(\"sentiment\",F.explode('sentiment.result'))\nresult.show()",
                    "submissionTime": "2023-11-16T05:31:29.371GMT",
                    "completionTime": "2023-11-16T05:31:29.629GMT",
                    "stageIds": [
                      19
                    ],
                    "jobGroup": "39",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "9362c820-0a88-4e32-a38f-43aa45857509"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 39, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n|     subreddit|               title|            selftext|year|month|            document| sentence_embeddings|sentiment|              tokens|     filtered_tokens|         rawFeatures|            features|\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n|       pokemon|the PokemonTogeth...|So several days a...|2023|    2|[{document, 0, 13...|[{sentence_embedd...|      pos|[so, several, day...|[several, days, a...|(262144,[3888,840...|(262144,[3888,840...|\n|       pokemon|Who's a non-villa...|For me, Tyme insp...|2023|    2|[{document, 0, 66...|[{sentence_embedd...|      neg|[for, me,, tyme, ...|[me,, tyme, inspi...|(262144,[3421,100...|(262144,[3421,100...|\n|       pokemon|i have a realization|&amp;#x200B;\\n\\n[...|2023|    2|[{document, 0, 18...|[{sentence_embedd...|      neg|[&amp;#x200b;, , ...|[&amp;#x200b;, , ...|(262144,[58503,13...|(262144,[58503,13...|\n|          FIFA|Is there any reas...|For the past 10 d...|2023|    2|[{document, 0, 63...|[{sentence_embedd...|      neg|[for, the, past, ...|[past, 10, days, ...|(262144,[1889,230...|(262144,[1889,230...|\n|           GTA|What should I buy...|I have around 5 m...|2023|    2|[{document, 0, 22...|[{sentence_embedd...|      neg|[i, have, around,...|[around, 5, mil, ...|(262144,[18697,28...|(262144,[18697,28...|\n|           GTA|what is the name ...|I know the Nero i...|2023|    2|[{document, 0, 95...|[{sentence_embedd...|      pos|[i, know, the, ne...|[know, nero, one,...|(262144,[21823,87...|(262144,[21823,87...|\n|       pokemon|Name any Bug type...|Ok now we’re doin...|2023|    2|[{document, 0, 42...|[{sentence_embedd...|      neg|[ok, now, we’re, ...|[ok, we’re, worst...|(262144,[10564,39...|(262144,[10564,39...|\n|       pokemon|My starters for e...|Gen 1: Charizard ...|2023|    2|[{document, 0, 30...|[{sentence_embedd...|      pos|[gen, 1:, chariza...|[gen, 1:, chariza...|(262144,[4959,129...|(262144,[4959,129...|\n|       thesims|The Victoria Chal...|\\n\\nI made my own...|2023|    2|[{document, 0, 43...|[{sentence_embedd...|      pos|[, , i, made, my,...|[, , made, challe...|(262144,[3436,353...|(262144,[3436,353...|\n|       pokemon|I really fucking ...|I feel like it's ...|2023|    2|[{document, 0, 37...|[{sentence_embedd...|      neg|[i, feel, like, i...|[feel, like, fuck...|(262144,[8254,208...|(262144,[8254,208...|\n|       thesims|The sim 4 build m...|So whenever I pla...|2023|    2|[{document, 0, 33...|[{sentence_embedd...|      neg|[so, whenever, i,...|[whenever, play, ...|(262144,[3530,157...|(262144,[3530,157...|\n|          FIFA|  flair passes trait|anyone know which...|2023|    2|[{document, 0, 13...|[{sentence_embedd...|      pos|[anyone, know, wh...|[anyone, know, pl...|(262144,[21823,28...|(262144,[21823,28...|\n|  legostarwars|Ghtroc 720 build ...|So I'm going to t...|2023|    2|[{document, 0, 41...|[{sentence_embedd...|      pos|[so, i'm, going, ...|[going, try, make...|(262144,[6821,853...|(262144,[6821,853...|\n|       pokemon|One of the follow...|I came up with th...|2023|    2|[{document, 0, 60...|[{sentence_embedd...|      pos|[i, came, up, wit...|[came, poll, walk...|(262144,[3184,443...|(262144,[3184,443...|\n|       pokemon|Emi's Daily Pokem...|[Art by me!](http...|2023|    2|[{document, 0, 39...|[{sentence_embedd...|      pos|[[art, by, me!](h...|[[art, me!](https...|(262144,[980,1578...|(262144,[980,1578...|\n|assassinscreed|Hot Take: Assassi...|AC Unity has been...|2023|    2|[{document, 0, 22...|[{sentence_embedd...|      pos|[ac, unity, has, ...|[ac, unity, getti...|(262144,[471,1376...|(262144,[471,1376...|\n|  FinalFantasy|Meme Monday has e...|Meme Monday has e...|2023|    2|[{document, 0, 14...|[{sentence_embedd...|      pos|[meme, monday, ha...|[meme, monday, en...|(262144,[32060,36...|(262144,[32060,36...|\n|       pokemon|First time EV tra...|I'm playing throu...|2023|    2|[{document, 0, 41...|[{sentence_embedd...|      neg|[i'm, playing, th...|[playing, scarlet...|(262144,[21823,26...|(262144,[21823,26...|\n|assassinscreed|I made a long ret...|I used to really ...|2023|    2|[{document, 0, 16...|[{sentence_embedd...|      pos|[i, used, to, rea...|[used, really, di...|(262144,[2120,347...|(262144,[2120,347...|\n|       pokemon|monotype run for ...|i have had shield...|2023|    2|[{document, 0, 27...|[{sentence_embedd...|      pos|[i, have, had, sh...|[shield, 2, years...|(262144,[12524,15...|(262144,[12524,15...|\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700112690678
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.select(\"filtered_tokens\").take(4)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 64,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T05:56:23.4147866Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T05:56:23.5195433Z",
              "execution_finish_time": "2023-11-16T05:56:25.2322845Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "take at /tmp/ipykernel_17302/635649604.py:1",
                    "dataWritten": 0,
                    "dataRead": 381392,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 36,
                    "name": "take at /tmp/ipykernel_17302/635649604.py:1",
                    "description": "Job group for statement 64:\nresult.select(\"filtered_tokens\").take(4)",
                    "submissionTime": "2023-11-16T05:56:23.595GMT",
                    "completionTime": "2023-11-16T05:56:23.714GMT",
                    "stageIds": [
                      40
                    ],
                    "jobGroup": "64",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "414dcce2-35b4-47f8-83cc-06b9432f5e0d"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 64, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 127,
          "data": {
            "text/plain": "[Row(filtered_tokens=['several', 'days', 'ago,', 'pokemon', 'company', 'put', 'campaign', 'celebration', 'pokemon', 'day', 'approaching', 'fans', 'submit', 'short', 'story', '', 'favourite', 'memories', 'related', 'pokemon', 'chance', 'submission', 'photo', 'appear', 'pokemon', 'day', 'mosaic', 'tag', 'tpci', 'use', 'hashtag', 'social', 'media.', '', 'yet,', 'please', 'soon!', 'have,', 'fond', 'memory', 'pokemon', 'series', 'experienced?', '', '', '', 'mine', 'pokemon', 'series', 'whole', 'inspired', 'pursue', 'biology', 'wayy', 'back', 'elementary', '/', 'middle', 'school.', 'loved', 'outdoors', 'activities', 'well', 'studying', 'different', 'kinds', 'animals', '(specifically', 'birds),', 'addition,', 'family', 'lived', 'right', 'next', 'untouched', 'woods', 'always', 'many', 'kinds', 'birds', 'younger', 'watch', 'learn', 'about!', '', 'currently', 'writing', 'this,', '2nd', 'yr', 'college', 'biotech', 'major', 'nothing', 'beats', 'opening', 'pokemon', 'game', 'exploring', \"region's\", 'different', 'ecosystems!', 'scarlet', \"violet's\", 'open', 'world', 'style', 'really', 'brought', 'sense', 'exploration', 'due', 'pokemon', 'felt', 'really', 'connected', 'certain', 'habitats', 'like', 'real', 'life......like', 'veluza', 'lake', 'chase', 'lol', '', 'please', 'share', 'fond', 'memories', 'pokemon', 'fun', 'experience', 'youve', 'related', 'pokemon', 'below!', 'love', 'hear', '!!']),\n Row(filtered_tokens=['me,', 'tyme', 'inspires', 'absolute', 'rage', 'me.', 'lectures', 'hassel', 'showing', 'normal', 'human', 'emotion', 'guess', 'dare', 'overjoyed', 'far', 'brassius', 'come', 'recovering', 'severe', 'depression.', 'insists', 'clavell', 'trying', 'protect', 'student', 'somehow', 'line.', 'resistant', '*own*', 'emotions', 'answer', 'students', 'stopped', 'gym', 'leader', 'makes', 'sad,', 'even', 'though', 'one', 'time', 'everyone', 'get', 'case', 'it.', '', 'really', 'makes', 'wonder', 'relationship', 'much', 'emotional', 'sister', 'like,', 'ryme', 'passionate', 'plot', 'point.']),\n Row(filtered_tokens=['&amp;#x200b;', '', '[brock', 'tall](https://preview.redd.it/k8wbk83t3tja1.png?width=1444&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4a527d66c0ce716546c0429432b5dc6c435f0b03)']),\n Row(filtered_tokens=['past', '10', 'days', 'ai', 'defence', 'feels', 'like', 'amateur', 'level,', 'players', 'dont', 'move', 'make', 'runs', 'whatsoever.', '', 'im', 'ball,', 'everything', 'feels', 'like', 'takes', 'forever,', 'movement', 'slow,', 'mbappe', 'runs', 'like', '70', 'pace', 'example.', 'cant', 'seem', 'able', 'precision', 'dribble', 'dribble', 'every', 'command', 'seems', 'take', 'long', 'enemy', 'team', 'already', 'tackled', 'me.', '', '', 'completely', 'different', 'unchanged', 'team', 'used', 'play', '2', 'weeks', 'ago,', 'without', 'changes', 'tactics', 'anything.', '', 'im', 'always', '24-38', 'ping', 'playing', 'europe', 'pc', '', 'ive', 'tried', 'playing', 'different', 'times', 'day', 'change', 'thing'])]"
          },
          "metadata": {}
        }
      ],
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700114185419
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "ndf = result.select(\"subreddit\",f.explode('filtered_tokens').name('expwords'),\"rawFeatures\",\"year\",\"month\",\"sentiment\").withColumn('filtered_tokens',f.array('expwords'))\n",
        "hashudf = f.udf(lambda vector : vector.indices.tolist()[0],StringType())\n",
        "wordtf = ndf.withColumn('wordhash', hashudf(col('rawFeatures')))\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "wordtf = wordtf.drop('rawFeatures')\n",
        "wordtf.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 55,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T05:47:27.5446583Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T05:47:27.6647505Z",
              "execution_finish_time": "2023-11-16T05:47:30.0513483Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 381392,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 32,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 55:\nfrom pyspark.sql import functions as f\nfrom pyspark.sql.types import MapType, StringType\nndf = result.select(\"subreddit\",f.explode('filtered_tokens').name('expwords'),\"rawFeatures\",\"year\",\"month\",\"sentiment\").withColumn('filtered_tokens',f.array('expwords'))\nhashudf = f.udf(lambda vector : vector.indices.tolist()[0],StringType())\nwordtf = ndf.withColumn('wordhash', hashudf(col('rawFeatures')))\n\n# Show the resulting DataFrame\nwordtf = wordtf.drop('rawFeatures')\nwordtf.show()",
                    "submissionTime": "2023-11-16T05:47:27.850GMT",
                    "completionTime": "2023-11-16T05:47:28.501GMT",
                    "stageIds": [
                      36
                    ],
                    "jobGroup": "55",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "125fe12c-3b4a-45bc-885d-57135b353092"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 55, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---------+-----------+----+-----+---------+---------------+--------+\n|subreddit|   expwords|year|month|sentiment|filtered_tokens|wordhash|\n+---------+-----------+----+-----+---------+---------------+--------+\n|  pokemon|    several|2023|    2|      pos|      [several]|    3888|\n|  pokemon|       days|2023|    2|      pos|         [days]|    3888|\n|  pokemon|       ago,|2023|    2|      pos|         [ago,]|    3888|\n|  pokemon|    pokemon|2023|    2|      pos|      [pokemon]|    3888|\n|  pokemon|    company|2023|    2|      pos|      [company]|    3888|\n|  pokemon|        put|2023|    2|      pos|          [put]|    3888|\n|  pokemon|   campaign|2023|    2|      pos|     [campaign]|    3888|\n|  pokemon|celebration|2023|    2|      pos|  [celebration]|    3888|\n|  pokemon|    pokemon|2023|    2|      pos|      [pokemon]|    3888|\n|  pokemon|        day|2023|    2|      pos|          [day]|    3888|\n|  pokemon|approaching|2023|    2|      pos|  [approaching]|    3888|\n|  pokemon|       fans|2023|    2|      pos|         [fans]|    3888|\n|  pokemon|     submit|2023|    2|      pos|       [submit]|    3888|\n|  pokemon|      short|2023|    2|      pos|        [short]|    3888|\n|  pokemon|      story|2023|    2|      pos|        [story]|    3888|\n|  pokemon|           |2023|    2|      pos|             []|    3888|\n|  pokemon|  favourite|2023|    2|      pos|    [favourite]|    3888|\n|  pokemon|   memories|2023|    2|      pos|     [memories]|    3888|\n|  pokemon|    related|2023|    2|      pos|      [related]|    3888|\n|  pokemon|    pokemon|2023|    2|      pos|      [pokemon]|    3888|\n+---------+-----------+----+-----+---------+---------------+--------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700113650198
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import SparseVector\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "#df_exploded = result.select(\"features\", \"filtered_tokens\",F.explode(\"filtered_tokens\").alias(\"token\"))\n",
        "# Define a UDF to extract values from SparseVector\n",
        "extract_values_udf = f.udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
        "\n",
        "# Apply the UDF to the DataFrame\n",
        "df_exploded = result.withColumn(\"feature_values\", extract_values_udf(\"features\"))\n",
        "#df_exploded = df_exploded.select(\"token\", F.explode(\"feature_dict\").alias(\"feature_key\", \"feature_value\"))\n",
        "# Show the exploded DataFrame\n",
        "df_exploded.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 81,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T06:15:20.8502673Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T06:15:20.9590957Z",
              "execution_finish_time": "2023-11-16T06:15:35.1419325Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 381392,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 45,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 81:\nfrom pyspark.ml.linalg import SparseVector\nfrom pyspark.sql.types import ArrayType, DoubleType\n#df_exploded = result.select(\"features\", \"filtered_tokens\",F.explode(\"filtered_tokens\").alias(\"token\"))\n# Define a UDF to extract values from SparseVector\nextract_values_udf = f.udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n\n# Apply the UDF to the DataFrame\ndf_exploded = result.withColumn(\"feature_values\", extract_values_udf(\"features\"))\n#df_exploded = df_exploded.select(\"token\", F.explode(\"feature_dict\").alias(\"feature_key\", \"feature_value\"))\n# Show the exploded DataFrame\ndf_exploded.show()",
                    "submissionTime": "2023-11-16T06:15:21.186GMT",
                    "completionTime": "2023-11-16T06:15:32.113GMT",
                    "stageIds": [
                      49
                    ],
                    "jobGroup": "81",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "75ea5951-a4d3-4b17-a379-4574cdb8be44"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 81, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|     subreddit|               title|            selftext|year|month|            document| sentence_embeddings|sentiment|              tokens|     filtered_tokens|         rawFeatures|            features|      feature_values|\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|       pokemon|the PokemonTogeth...|So several days a...|2023|    2|[{document, 0, 13...|[{sentence_embedd...|      pos|[so, several, day...|[several, days, a...|(262144,[3888,840...|(262144,[3888,840...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|Who's a non-villa...|For me, Tyme insp...|2023|    2|[{document, 0, 66...|[{sentence_embedd...|      neg|[for, me,, tyme, ...|[me,, tyme, inspi...|(262144,[3421,100...|(262144,[3421,100...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|i have a realization|&amp;#x200B;\\n\\n[...|2023|    2|[{document, 0, 18...|[{sentence_embedd...|      neg|[&amp;#x200b;, , ...|[&amp;#x200b;, , ...|(262144,[58503,13...|(262144,[58503,13...|[0.0, 0.0, 0.0, 0...|\n|          FIFA|Is there any reas...|For the past 10 d...|2023|    2|[{document, 0, 63...|[{sentence_embedd...|      neg|[for, the, past, ...|[past, 10, days, ...|(262144,[1889,230...|(262144,[1889,230...|[0.0, 0.0, 0.0, 0...|\n|           GTA|What should I buy...|I have around 5 m...|2023|    2|[{document, 0, 22...|[{sentence_embedd...|      neg|[i, have, around,...|[around, 5, mil, ...|(262144,[18697,28...|(262144,[18697,28...|[0.0, 0.0, 0.0, 0...|\n|           GTA|what is the name ...|I know the Nero i...|2023|    2|[{document, 0, 95...|[{sentence_embedd...|      pos|[i, know, the, ne...|[know, nero, one,...|(262144,[21823,87...|(262144,[21823,87...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|Name any Bug type...|Ok now we’re doin...|2023|    2|[{document, 0, 42...|[{sentence_embedd...|      neg|[ok, now, we’re, ...|[ok, we’re, worst...|(262144,[10564,39...|(262144,[10564,39...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|My starters for e...|Gen 1: Charizard ...|2023|    2|[{document, 0, 30...|[{sentence_embedd...|      pos|[gen, 1:, chariza...|[gen, 1:, chariza...|(262144,[4959,129...|(262144,[4959,129...|[0.0, 0.0, 0.0, 0...|\n|       thesims|The Victoria Chal...|\\n\\nI made my own...|2023|    2|[{document, 0, 43...|[{sentence_embedd...|      pos|[, , i, made, my,...|[, , made, challe...|(262144,[3436,353...|(262144,[3436,353...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|I really fucking ...|I feel like it's ...|2023|    2|[{document, 0, 37...|[{sentence_embedd...|      neg|[i, feel, like, i...|[feel, like, fuck...|(262144,[8254,208...|(262144,[8254,208...|[0.0, 0.0, 0.0, 0...|\n|       thesims|The sim 4 build m...|So whenever I pla...|2023|    2|[{document, 0, 33...|[{sentence_embedd...|      neg|[so, whenever, i,...|[whenever, play, ...|(262144,[3530,157...|(262144,[3530,157...|[0.0, 0.0, 0.0, 0...|\n|          FIFA|  flair passes trait|anyone know which...|2023|    2|[{document, 0, 13...|[{sentence_embedd...|      pos|[anyone, know, wh...|[anyone, know, pl...|(262144,[21823,28...|(262144,[21823,28...|[0.0, 0.0, 0.0, 0...|\n|  legostarwars|Ghtroc 720 build ...|So I'm going to t...|2023|    2|[{document, 0, 41...|[{sentence_embedd...|      pos|[so, i'm, going, ...|[going, try, make...|(262144,[6821,853...|(262144,[6821,853...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|One of the follow...|I came up with th...|2023|    2|[{document, 0, 60...|[{sentence_embedd...|      pos|[i, came, up, wit...|[came, poll, walk...|(262144,[3184,443...|(262144,[3184,443...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|Emi's Daily Pokem...|[Art by me!](http...|2023|    2|[{document, 0, 39...|[{sentence_embedd...|      pos|[[art, by, me!](h...|[[art, me!](https...|(262144,[980,1578...|(262144,[980,1578...|[0.0, 0.0, 0.0, 0...|\n|assassinscreed|Hot Take: Assassi...|AC Unity has been...|2023|    2|[{document, 0, 22...|[{sentence_embedd...|      pos|[ac, unity, has, ...|[ac, unity, getti...|(262144,[471,1376...|(262144,[471,1376...|[0.0, 0.0, 0.0, 0...|\n|  FinalFantasy|Meme Monday has e...|Meme Monday has e...|2023|    2|[{document, 0, 14...|[{sentence_embedd...|      pos|[meme, monday, ha...|[meme, monday, en...|(262144,[32060,36...|(262144,[32060,36...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|First time EV tra...|I'm playing throu...|2023|    2|[{document, 0, 41...|[{sentence_embedd...|      neg|[i'm, playing, th...|[playing, scarlet...|(262144,[21823,26...|(262144,[21823,26...|[0.0, 0.0, 0.0, 0...|\n|assassinscreed|I made a long ret...|I used to really ...|2023|    2|[{document, 0, 16...|[{sentence_embedd...|      pos|[i, used, to, rea...|[used, really, di...|(262144,[2120,347...|(262144,[2120,347...|[0.0, 0.0, 0.0, 0...|\n|       pokemon|monotype run for ...|i have had shield...|2023|    2|[{document, 0, 27...|[{sentence_embedd...|      pos|[i, have, had, sh...|[shield, 2, years...|(262144,[12524,15...|(262144,[12524,15...|[0.0, 0.0, 0.0, 0...|\n+--------------+--------------------+--------------------+----+-----+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 77,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700115335276
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "udf1 = f.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
        "valuedf = result.select('subreddit','filtered_tokens',f.explode(udf1(f.col('features'))).name('wordhash','value'))\n",
        "#valuedf = valuedf.withColumn(\"filtered_tokens\",f.explode(\"filtered_tokens\").alias(\"tokens\"))\n",
        "valuedf.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 83,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T06:17:16.9968751Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T06:17:17.120332Z",
              "execution_finish_time": "2023-11-16T06:17:17.9209996Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 1,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at <unknown>:0",
                    "dataWritten": 0,
                    "dataRead": 381392,
                    "rowCount": 1,
                    "usageDescription": "",
                    "jobId": 47,
                    "name": "showString at <unknown>:0",
                    "description": "Job group for statement 83:\n\nudf1 = f.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\nvaluedf = result.select('subreddit','filtered_tokens',f.explode(udf1(f.col('features'))).name('wordhash','value'))\n#valuedf = valuedf.withColumn(\"filtered_tokens\",f.explode(\"filtered_tokens\").alias(\"tokens\"))\nvaluedf.show()",
                    "submissionTime": "2023-11-16T06:17:17.233GMT",
                    "completionTime": "2023-11-16T06:17:17.672GMT",
                    "stageIds": [
                      51
                    ],
                    "jobGroup": "83",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "cbe97ec0-fa26-49c8-b46a-fbedfb0b84c4"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 83, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+---------+--------------------+--------+------------------+\n|subreddit|     filtered_tokens|wordhash|             value|\n+---------+--------------------+--------+------------------+\n|  pokemon|[several, days, a...|  196098| 5.764472278598879|\n|  pokemon|[several, days, a...|  116996|3.4225891399086903|\n|  pokemon|[several, days, a...|  111370|3.7956553068448686|\n|  pokemon|[several, days, a...|   73740| 7.329341871553103|\n|  pokemon|[several, days, a...|  186381| 6.062033192055356|\n|  pokemon|[several, days, a...|  151058|4.6907425222526955|\n|  pokemon|[several, days, a...|  241691| 6.263305753319606|\n|  pokemon|[several, days, a...|   13340| 13.01980193133773|\n|  pokemon|[several, days, a...|   93729|  7.63426575043939|\n|  pokemon|[several, days, a...|  201511| 4.043526564009793|\n|  pokemon|[several, days, a...|  205861|  6.79194922040239|\n|  pokemon|[several, days, a...|  229166| 3.008696354702516|\n|  pokemon|[several, days, a...|  186925| 2.998777236454856|\n|  pokemon|[several, days, a...|   23087|3.1829564312059584|\n|  pokemon|[several, days, a...|    3888| 7.835903096532636|\n|  pokemon|[several, days, a...|   35633| 7.444723945671948|\n|  pokemon|[several, days, a...|  261937| 10.76975996636854|\n|  pokemon|[several, days, a...|   31027| 6.258900459851689|\n|  pokemon|[several, days, a...|  192310| 4.304548803735988|\n|  pokemon|[several, days, a...|  169527|3.5831615636018257|\n+---------+--------------------+--------+------------------+\nonly showing top 20 rows\n\n"
        }
      ],
      "execution_count": 79,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700115438087
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "window_spec = Window.partitionBy(\"filtered_tokens\")\n",
        "window_spec.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 86,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T06:21:05.4755563Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T06:21:05.5789279Z",
              "execution_finish_time": "2023-11-16T06:21:08.8010403Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "c4b5a783-8353-4b6f-b56f-9af1f11a4371"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 86, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "cannot resolve 'explode(word_info)' due to data type mismatch: input to function explode should be array or map type, not struct<filtered_tokens:array<string>,wordhash:string,value:string>;\n'Project [subreddit#1457, explode(word_info#14152) AS word_info#14158]\n+- Project [subreddit#1457, filtered_tokens#1988, wordhash#13610, value#13611, struct(filtered_tokens, filtered_tokens#1988, wordhash, wordhash#13610, value, value#13611) AS word_info#14152]\n   +- Project [subreddit#1457, filtered_tokens#1988, wordhash#13610, value#13611]\n      +- Generate explode(<lambda>(features#2019)), false, [wordhash#13610, value#13611]\n         +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#2924, tokens#1972, filtered_tokens#1988, rawFeatures#2004, features#2019]\n            +- Generate explode(sentiment#1531.result), false, [sentiment#2924]\n               +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#1531, tokens#1972, filtered_tokens#1988, rawFeatures#2004, UDF(rawFeatures#2004) AS features#2019]\n                  +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#1531, tokens#1972, filtered_tokens#1988, UDF(filtered_tokens#1988) AS rawFeatures#2004]\n                     +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#1531, tokens#1972, UDF(tokens#1972) AS filtered_tokens#1988]\n                        +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#1531, UDF(selftext#1475) AS tokens#1972]\n                           +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, UDF(array(sentence_embeddings#1520)) AS sentiment#1531]\n                              +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1511 AS sentence_embeddings#1520]\n                                 +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1455 AS sentence_embeddings#1511]\n                                    +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1454 AS document#1502, sentence_embeddings#1455]\n                                       +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1453 AS month#1493, document#1454, sentence_embeddings#1455]\n                                          +- Project [subreddit#1457, title#1466, selftext#1475, year#1452 AS year#1484, month#1453, document#1454, sentence_embeddings#1455]\n                                             +- Project [subreddit#1457, title#1466, selftext#1451 AS selftext#1475, year#1452, month#1453, document#1454, sentence_embeddings#1455]\n                                                +- Project [subreddit#1457, title#1450 AS title#1466, selftext#1451, year#1452, month#1453, document#1454, sentence_embeddings#1455]\n                                                   +- Project [subreddit#1449 AS subreddit#1457, title#1450, selftext#1451, year#1452, month#1453, document#1454, sentence_embeddings#1455]\n                                                      +- SerializeFromObject [if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, subreddit), StringType), true, false, true) AS subreddit#1449, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, title), StringType), true, false, true) AS title#1450, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, selftext), StringType), true, false, true) AS selftext#1451, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, year), IntegerType) AS year#1452, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, month), IntegerType) AS month#1453, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), if (isnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)))) null else named_struct(annotatorType, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 0, annotatorType), StringType), true, false, true), begin, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 1, begin), IntegerType), end, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 2, end), IntegerType), result, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 3, result), StringType), true, false, true), metadata, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else newInstance(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData), embeddings, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayData, ArrayType(FloatType,false), toArrayData, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 5, embeddings), ArrayType(FloatType,false)), true, false, true)), validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, document), ArrayType(StructType(StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)),true)), None) AS document#1454, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), if (isnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)))) null else named_struct(annotatorType, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 0, annotatorType), StringType), true, false, true), begin, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 1, begin), IntegerType), end, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 2, end), IntegerType), result, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 3, result), StringType), true, false, true), metadata, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else newInstance(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData), embeddings, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayData, ArrayType(FloatType,false), toArrayData, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 5, embeddings), ArrayType(FloatType,false)), true, false, true)), validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, sentence_embeddings), ArrayType(StructType(StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)),true)), None) AS sentence_embeddings#1455]\n                                                         +- MapPartitions com.johnsnowlabs.nlp.AnnotatorModel$$Lambda$5818/525231699@3f8efc78, obj#1448: org.apache.spark.sql.Row\n                                                            +- DeserializeToObject createexternalrow(subreddit#102.toString, title#111.toString, selftext#99.toString, year#114, month#115, staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279), if (isnull(lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279))) null else createexternalrow(if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).annotatorType.toString, if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).begin, if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).end, if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).result.toString, if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData$, ObjectType(interface scala.collection.Map), toScalaMap, staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObject, StringType, true, 1280), lambdavariable(MapObject, StringType, true, 1280).toString, lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).metadata.keyArray, None).array, true, false, true), staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObject, StringType, true, 1281), lambdavariable(MapObject, StringType, true, 1281).toString, lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).metadata.valueArray, None).array, true, false, true), true, false, true), if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObject, FloatType, true, 1282), lambdavariable(MapObject, FloatType, true, 1282), lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).embeddings, None).array, true, false, true), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), document#1430, None).array, true, false, true), StructField(subreddit,StringType,true), StructField(title,StringType,true), StructField(selftext,StringType,true), StructField(year,IntegerType,true), StructField(month,IntegerType,true), StructField(document,ArrayType(StructType(StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)),true),true)), obj#1447: org.apache.spark.sql.Row\n                                                               +- Project [subreddit#102, title#111, selftext#99, year#114, month#115, UDF(selftext#99) AS document#1430]\n                                                                  +- Project [subreddit#102, title#111, selftext#99, year#114, month#115]\n                                                                     +- Filter subreddit#102 IN (Tetris,pokemon,SuperMario,GTA,CallOfDuty,FIFA,legostarwars,assassinscreed,thesims,FinalFantasy)\n                                                                        +- Filter (((length(selftext#99) > 0) AND NOT (selftext#99 = [deleted])) AND NOT (selftext#99 = [removed]))\n                                                                           +- Relation [adserver_click_url#46,adserver_imp_pixel#47,archived#48,author#49,author_cakeday#50,author_flair_css_class#51,author_flair_text#52,author_id#53,brand_safe#54,contest_mode#55,created_utc#56,crosspost_parent#57,crosspost_parent_list#58,disable_comments#59,distinguished#60,domain#61,domain_override#62,edited#63,embed_type#64,embed_url#65,gilded#66L,hidden#67,hide_score#68,href_url#69,... 46 more fields] parquet\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_17302/2394404659.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Explode the struct column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_exploded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word_info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word_info\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Extract individual columns from the struct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \"\"\"\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'explode(word_info)' due to data type mismatch: input to function explode should be array or map type, not struct<filtered_tokens:array<string>,wordhash:string,value:string>;\n'Project [subreddit#1457, explode(word_info#14152) AS word_info#14158]\n+- Project [subreddit#1457, filtered_tokens#1988, wordhash#13610, value#13611, struct(filtered_tokens, filtered_tokens#1988, wordhash, wordhash#13610, value, value#13611) AS word_info#14152]\n   +- Project [subreddit#1457, filtered_tokens#1988, wordhash#13610, value#13611]\n      +- Generate explode(<lambda>(features#2019)), false, [wordhash#13610, value#13611]\n         +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#2924, tokens#1972, filtered_tokens#1988, rawFeatures#2004, features#2019]\n            +- Generate explode(sentiment#1531.result), false, [sentiment#2924]\n               +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#1531, tokens#1972, filtered_tokens#1988, rawFeatures#2004, UDF(rawFeatures#2004) AS features#2019]\n                  +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#1531, tokens#1972, filtered_tokens#1988, UDF(filtered_tokens#1988) AS rawFeatures#2004]\n                     +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#1531, tokens#1972, UDF(tokens#1972) AS filtered_tokens#1988]\n                        +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, sentiment#1531, UDF(selftext#1475) AS tokens#1972]\n                           +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1520, UDF(array(sentence_embeddings#1520)) AS sentiment#1531]\n                              +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1511 AS sentence_embeddings#1520]\n                                 +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1502, sentence_embeddings#1455 AS sentence_embeddings#1511]\n                                    +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1493, document#1454 AS document#1502, sentence_embeddings#1455]\n                                       +- Project [subreddit#1457, title#1466, selftext#1475, year#1484, month#1453 AS month#1493, document#1454, sentence_embeddings#1455]\n                                          +- Project [subreddit#1457, title#1466, selftext#1475, year#1452 AS year#1484, month#1453, document#1454, sentence_embeddings#1455]\n                                             +- Project [subreddit#1457, title#1466, selftext#1451 AS selftext#1475, year#1452, month#1453, document#1454, sentence_embeddings#1455]\n                                                +- Project [subreddit#1457, title#1450 AS title#1466, selftext#1451, year#1452, month#1453, document#1454, sentence_embeddings#1455]\n                                                   +- Project [subreddit#1449 AS subreddit#1457, title#1450, selftext#1451, year#1452, month#1453, document#1454, sentence_embeddings#1455]\n                                                      +- SerializeFromObject [if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 0, subreddit), StringType), true, false, true) AS subreddit#1449, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, title), StringType), true, false, true) AS title#1450, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 2, selftext), StringType), true, false, true) AS selftext#1451, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 3, year), IntegerType) AS year#1452, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 4, month), IntegerType) AS month#1453, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), if (isnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)))) null else named_struct(annotatorType, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 0, annotatorType), StringType), true, false, true), begin, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 1, begin), IntegerType), end, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 2, end), IntegerType), result, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 3, result), StringType), true, false, true), metadata, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else newInstance(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData), embeddings, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayData, ArrayType(FloatType,false), toArrayData, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1283), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 5, embeddings), ArrayType(FloatType,false)), true, false, true)), validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 5, document), ArrayType(StructType(StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)),true)), None) AS document#1454, if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), if (isnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)))) null else named_struct(annotatorType, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 0, annotatorType), StringType), true, false, true), begin, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 1, begin), IntegerType), end, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 2, end), IntegerType), result, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 3, result), StringType), true, false, true), metadata, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else newInstance(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData), embeddings, if (validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayData, ArrayType(FloatType,false), toArrayData, validateexternaltype(getexternalrowfield(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, 1286), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), 5, embeddings), ArrayType(FloatType,false)), true, false, true)), validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 6, sentence_embeddings), ArrayType(StructType(StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)),true)), None) AS sentence_embeddings#1455]\n                                                         +- MapPartitions com.johnsnowlabs.nlp.AnnotatorModel$$Lambda$5818/525231699@3f8efc78, obj#1448: org.apache.spark.sql.Row\n                                                            +- DeserializeToObject createexternalrow(subreddit#102.toString, title#111.toString, selftext#99.toString, year#114, month#115, staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279), if (isnull(lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279))) null else createexternalrow(if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).annotatorType.toString, if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).begin, if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).end, if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).result.toString, if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.ArrayBasedMapData$, ObjectType(interface scala.collection.Map), toScalaMap, staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObject, StringType, true, 1280), lambdavariable(MapObject, StringType, true, 1280).toString, lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).metadata.keyArray, None).array, true, false, true), staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObject, StringType, true, 1281), lambdavariable(MapObject, StringType, true, 1281).toString, lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).metadata.valueArray, None).array, true, false, true), true, false, true), if (lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).isNullAt) null else staticinvoke(class scala.collection.mutable.WrappedArray$, ObjectType(interface scala.collection.Seq), make, mapobjects(lambdavariable(MapObject, FloatType, true, 1282), lambdavariable(MapObject, FloatType, true, 1282), lambdavariable(MapObject, StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true), true, 1279).embeddings, None).array, true, false, true), StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)), document#1430, None).array, true, false, true), StructField(subreddit,StringType,true), StructField(title,StringType,true), StructField(selftext,StringType,true), StructField(year,IntegerType,true), StructField(month,IntegerType,true), StructField(document,ArrayType(StructType(StructField(annotatorType,StringType,true), StructField(begin,IntegerType,false), StructField(end,IntegerType,false), StructField(result,StringType,true), StructField(metadata,MapType(StringType,StringType,true),true), StructField(embeddings,ArrayType(FloatType,false),true)),true),true)), obj#1447: org.apache.spark.sql.Row\n                                                               +- Project [subreddit#102, title#111, selftext#99, year#114, month#115, UDF(selftext#99) AS document#1430]\n                                                                  +- Project [subreddit#102, title#111, selftext#99, year#114, month#115]\n                                                                     +- Filter subreddit#102 IN (Tetris,pokemon,SuperMario,GTA,CallOfDuty,FIFA,legostarwars,assassinscreed,thesims,FinalFantasy)\n                                                                        +- Filter (((length(selftext#99) > 0) AND NOT (selftext#99 = [deleted])) AND NOT (selftext#99 = [removed]))\n                                                                           +- Relation [adserver_click_url#46,adserver_imp_pixel#47,archived#48,author#49,author_cakeday#50,author_flair_css_class#51,author_flair_text#52,author_id#53,brand_safe#54,contest_mode#55,created_utc#56,crosspost_parent#57,crosspost_parent_list#58,disable_comments#59,distinguished#60,domain#61,domain_override#62,edited#63,embed_type#64,embed_url#65,gilded#66L,hidden#67,hide_score#68,href_url#69,... 46 more fields] parquet\n"
          ]
        }
      ],
      "execution_count": 82,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700115668957
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, desc, collect_list, sort_array\n",
        "from pyspark.sql import functions as F\n",
        "res = valuedf.withColumn(\n",
        "    \"value\",\n",
        "    F.first(\"value\").over(F.window(\"subreddit\", \"filtered_tokens\"))\n",
        ").withColumn(\n",
        "    \"wordhash\",\n",
        "    F.first(\"wordhash\").over(F.window(\"subreddit\", \"filtered_tokens\"))\n",
        ")\n",
        "\n",
        "res.show(truncate=False)\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 58,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-11-16T05:52:59.7156405Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T05:52:59.8114862Z",
              "execution_finish_time": "2023-11-16T05:53:00.1024435Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 0,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "4ede551a-0f8d-4f9f-b573-8de147fecde4"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 58, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Unable to parse 'filtered_tokens'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_17302/348426429.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m res = valuedf.withColumn(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"filtered_tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"wordhash\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mwindow\u001b[0;34m(timeColumn, windowDuration, slideDuration, startTime)\u001b[0m\n\u001b[1;32m   2352\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindowDuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindowDuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2354\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindowDuration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2355\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/cluster-env/env/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Unable to parse 'filtered_tokens'"
          ]
        }
      ],
      "execution_count": 54,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700113980203
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_without_duplicates = joined_df.dropDuplicates()\n",
        "\n",
        "# Show the resulting DataFrame without duplicates\n",
        "result_without_duplicates.cache().show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "3c67b279-1d53-4b7a-b0d9-41cb8b4b6723",
              "session_id": "20",
              "statement_id": 48,
              "state": "cancelled",
              "livy_statement_state": "waiting",
              "queued_time": "2023-11-16T05:43:33.0017816Z",
              "session_start_time": null,
              "execution_start_time": "2023-11-16T05:43:34.0907422Z",
              "execution_finish_time": "2023-11-16T05:44:52.773545Z",
              "spark_jobs": {
                "numbers": {
                  "SUCCEEDED": 0,
                  "RUNNING": 1,
                  "FAILED": 0,
                  "UNKNOWN": 0
                },
                "jobs": [
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 19847824,
                    "dataRead": 88999008,
                    "rowCount": 322817,
                    "usageDescription": "",
                    "jobId": 26,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 48:\nresult_without_duplicates = joined_df.dropDuplicates()\n\n# Show the resulting DataFrame without duplicates\nresult_without_duplicates.cache().show()",
                    "submissionTime": "2023-11-16T05:43:34.365GMT",
                    "stageIds": [
                      30,
                      29
                    ],
                    "jobGroup": "48",
                    "status": "RUNNING",
                    "numTasks": 3081,
                    "numActiveTasks": 9,
                    "numCompletedTasks": 29,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 29,
                    "numActiveStages": 1,
                    "numCompletedStages": 0,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "f90206bc-d562-4a99-8f31-c2e0d272a8ed"
            },
            "text/plain": "StatementMeta(3c67b279-1d53-4b7a-b0d9-41cb8b4b6723, 20, 48, Cancelled, Waiting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 44,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1700113492749
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving intermediate data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intermediate outputs go into the azureml workspace attached storage using the URI `azureml://datastores/workspaceblobstore/paths/<PATH-TO_STORE>` this is the same for all workspaces. Then to re-load you use the same URI"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "CSV_DIR = os.path.join(\"Users/yc1063/fall-2023-reddit-project-team-11/data\", \"csv\")\n",
        "joined_df.write.parquet(f\"{CSV_DIR}/sentiment_tfidf.csv\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "cf47b043-0c98-4514-b15a-1e3237e3aed3",
              "session_id": "23",
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2023-10-17T22:05:35.6813864Z",
              "session_start_time": null,
              "execution_start_time": "2023-10-17T22:05:35.7781388Z",
              "execution_finish_time": "2023-10-17T22:05:46.2845543Z",
              "spark_jobs": {
                "numbers": {
                  "RUNNING": 0,
                  "UNKNOWN": 0,
                  "SUCCEEDED": 1,
                  "FAILED": 0
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 17748445,
                    "dataRead": 10524944,
                    "rowCount": 194820,
                    "usageDescription": "",
                    "jobId": 2,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 13:\ndatastore = 'azureml://datastores/workspaceblobstore/paths'\ncomments_single_df.write.parquet(f\"{datastore}/memes.parquet\")",
                    "submissionTime": "2023-10-17T22:05:37.990GMT",
                    "completionTime": "2023-10-17T22:05:44.762GMT",
                    "stageIds": [
                      2
                    ],
                    "jobGroup": "13",
                    "status": "SUCCEEDED",
                    "numTasks": 3,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 3,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 3,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "parent_msg_id": "aa685af2-ce8a-402c-95e5-cbf7983dc9b5"
            },
            "text/plain": "StatementMeta(cf47b043-0c98-4514-b15a-1e3237e3aed3, 23, 13, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1697580345876
        }
      }
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Comments JSON to Parquet",
      "widgets": {}
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}